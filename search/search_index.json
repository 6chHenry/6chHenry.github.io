{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u4e3b\u9875","text":"Welcome to 6ch.'s Site! \ud83c\udf89  <p>  About Me /   Academic Page /  Statistics </p> <li>Website Operating Time: </li> <li>Total Visitors:  people</li> <li>Total Visits:  times</li>"},{"location":"academy/","title":"KeWei Liu(\u5218\u53ef\u552f)","text":""},{"location":"academy/#kewei-liu","title":"KeWei Liu(\u5218\u53ef\u552f)","text":"<p> Work Email: HenryLiu</p> <p> Personal Email: 2313287840 [at] qq [dot] com</p> <p> CV: Empty</p> <p></p>"},{"location":"academy/#bio","title":"Bio","text":"<p>I am a first-year undergraduate student majoring in Mechanics at the ME of Shanghai Jiao Tong University (SJTU). Currently, I am an active participant in Zhiyuan Honors Program, which fosters academic excellence and innovation.</p> <p>I am actively seeking internships in the field of machine learning systems and optimization. Please feel free to reach out to me if you have any opportunities available! \ud83e\udd70\ud83e\udd70\ud83e\udd70</p>"},{"location":"academy/#research-interest","title":"Research Interest","text":"<ul> <li>Machine Learning Systems for Advancing AI and AGI: I am committed to the development of robust and efficient machine learning systems, recognizing that the advancement of AI and AGI is fundamentally constrained by the capabilities of their underlying system infrastructures.</li> <li>Optimization in Large Model Training and Reasoning: I also focus on improving the efficiency and practicality of large-scale model training and inference through advanced algorithms.</li> </ul>"},{"location":"academy/#news","title":"News","text":"20252024 <p>[02/2025] I became a member of Zhiyuan Honor Program,SJTU.</p> <p>[09/2024] Excited to start my ME B.Eng. at SJTU ME.</p>"},{"location":"academy/#education","title":"Education","text":""},{"location":"academy/#zhiyuan-honor-program-shanghai-jiao-tong-university","title":"Zhiyuan Honor Program, Shanghai Jiao Tong University","text":"<p>Feb. 2025 -- Present</p> <p></p>"},{"location":"academy/#school-of-mechanical-engineering-shanghai-jiao-tong-university","title":"School of Mechanical Engineering, Shanghai Jiao Tong University","text":"<p>Sept. 2024 -- Present</p>"},{"location":"academy/#publications-manuscripts","title":"Publications &amp; Manuscripts","text":"<p>Coming soon...</p>"},{"location":"academy/#experience","title":"Experience","text":""},{"location":"academy/#projects","title":"Projects","text":"<p>The page is powered by Material for MkDocs and supports collaboration through Pull Requests and GitHub Actions.</p>"},{"location":"academy/#media-exposures","title":"Media Exposures","text":""},{"location":"academy/#honors","title":"Honors","text":""},{"location":"about/","title":"\u5173\u4e8e\u6211","text":""},{"location":"about/#about","title":"About \ud83e\udd73","text":"<p> \u7ea6 409 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> \u4e2d\u6587English <p>\u4f60\u597d\uff01\u6211\u662f 6ch.\uff0c\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 24 \u7ea7\u673a\u68b0\u7c7b\u4e13\u4e1a\u7684\u672c\u79d1\u751f\u3002\u5f88\u9ad8\u5174\u80fd\u5728\u4e92\u8054\u7f51\u4e0a\u4e0e\u4f60\u76f8\u9047\ud83e\udd70\u3002</p> <p>\u6211\u5bf9\u6df1\u5ea6\u5b66\u4e60\u3001\u5927\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7b49\u9886\u57df\u5f88\u6709\u70ed\u60c5\uff0c\u4e5f\u559c\u6b22\u5206\u4eab\u81ea\u5df1\u7684\u5b66\u4e60\u7ecf\u9a8c\u548c\u601d\u8003\ud83e\udd13\u3002</p> <p>\u60f3\u4e86\u89e3\u6211\u7684\u5b66\u672f\u7ecf\u5386\uff1f\u53ef\u4ee5\u67e5\u770b\u6211\u7684\u5b66\u672f\u4e3b\u9875\ud83c\udf93\u3002</p> <p>\u5982\u679c\u6709\u4efb\u4f55\u95ee\u9898\uff0c\u6216\u8005\u60f3\u548c\u6211\u4ea4\u6d41\uff0c\u6b22\u8fce\u53d1\u90ae\u4ef6\u6216\u8005\u76f4\u63a5\u5728\u4e0b\u65b9\u7559\u8a00\uff0c\u6211\u4f1a\u5c3d\u5feb\u56de\u590d\ud83d\ude0e\u3002  \u5982\u679c\u4f60\u4e5f\u5728\u4e0a\u6d77\uff0c\u6b22\u8fce\u8054\u7cfb\u6211\u3002\u6211\u4eec\u53ef\u4ee5\u4e00\u8d77\u804a\u5929\u3001\u5b66\u4e60\uff0c\u6216\u8005\u4ea4\u6d41\u5171\u540c\u7684\u5174\u8da3\u7231\u597d\ud83d\udc7b\u3002</p> <p>GitHub \u70b9\u4e2a\u5173\u6ce8\u8c22\u8c22\u55b5\ud83d\ude3a\uff0cGitHub \u70b9\u4e2a\u5173\u6ce8\u8c22\u8c22\u55b5\ud83d\ude3a</p> <p>\u6211\u5e73\u65f6\u559c\u6b22\u542c\u97f3\u4e50(Eason!)\u3001\u9605\u8bfb\u3001\u6253\u7fbd\u6bdb\u7403\uff0c\u4e5f\u4f1a\u5728\u65e5\u8bb0\u4e2d\u8bb0\u5f55\u4e00\u4e9b\u76f8\u5173\u5185\u5bb9\uff0c\u5206\u4eab\u4e2a\u4eba\u611f\u53d7\u270d\u3002</p> <p>\u81f3\u4e8e\u6211\u7684\u540d\u5b57\u201c6ch.\u201d\u561b\uff0c\u89e3\u91ca\u8d77\u6765\u592a\u56f0\u96be\u4e86\uff0c\u5c31\u5f53\u662f\u6211\u968f\u4fbf\u53d6\u7684\u597d\u5566\uff01</p> <p>Hello. I'm 6ch! I'm 6ch. and I'm an undergraduate student majoring in Mechanics in the 24<sup>th</sup> grade at Shanghai Jiaotong University. I'm glad to meet you on the internet \ud83e\udd70.</p> <p>I am passionate about the fields of deep learning, big models and machine learning systems, and I like to share my learning experience and thinking \ud83e\udd13.</p> <p>Want to know more about my academic experience? Check out my [academic homepage \ud83c\udf93] (.. /academy.md).</p> <p>If you have any questions or would like to communicate with me, please feel free to email or just leave a message below and I will reply as soon as possible \ud83d\ude0e. If you are also in Shanghai, feel free to contact me. We can chat, learn, or exchange common interests together \ud83d\udc7b.</p> <p>Follow my github thank you meow \ud83d\ude3a, follow my Github thank you meow \ud83d\ude3a~</p> <p>I usually like listening to music(Eason!), reading and playing badminton, and I also write diaries about it and share my personal feelings \u270d.</p> <p>As for my name \u201c6ch.\u201d, it's a long long story.</p>"},{"location":"diaries/","title":"index","text":""},{"location":"diaries/#diaries","title":"Diaries \u270d","text":"<p>Abstract</p> <p>\u4e2a\u4eba\u65e5\u8bb0\uff0c\u4e3b\u8981\u8bb0\u5f55</p> <ul> <li>\u751f\u6d3b\u611f\u609f(Just Be Happy!)\uff1b</li> <li>\u8bfb\u4e66\u6458\u5f55\uff0c\u53ef\u80fd\u4f1a\u6709\u4e00\u4e9b\u7b14\u8bb0\uff1b</li> <li>\u4e00\u4e9b\u6742\u8c08\u3002</li> </ul> <p>\u4e00\u4e9b\u6bd4\u8f83\u6210\u4f53\u7cfb\u7684\u7b14\u8bb0\u4f1a\u8bb0\u5f55\u5728 Notes \u4e2d\u3002</p> <p>\u672c\u90e8\u5206\u5185\u5bb9\uff08\u9664\u7279\u522b\u58f0\u660e\u5916\uff09\u91c7\u7528 \u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u4fdd\u6301\u4e00\u81f4 4.0 \u56fd\u9645 (CC BY-NC-SA 4.0) \u8bb8\u53ef\u534f\u8bae\u8fdb\u884c\u8bb8\u53ef\u3002</p>"},{"location":"diaries/#archives","title":"Archives","text":"<p>\u5982\u679c\u5bfb\u627e\u4e0d\u65b9\u4fbf\u7684\u8bdd\uff0c\u4e0d\u59a8\u8bd5\u8bd5\u641c\u7d22</p>"},{"location":"diaries/2024/20241226/","title":"20241226","text":"<p> \u7ea6 58 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u4e3b\u8981\u4efb\u52a1\uff1a</p> <ul> <li> 1.\u68b3\u7406\u671f\u672b\u8003\u8bd5\u590d\u4e60\u8ba1\u5212  </li> <li> 2.\u68b3\u7406\u5237\u8bfe\u8fdb\u5ea6\u5b89\u6392</li> <li> 3.\u5b8c\u6210\u6570\u5206\u4f5c\u4e1a</li> <li> 4.\u590d\u4e60\u82f1\u8bedquiz</li> <li> 5.\u505a\u8c22\u60e0\u6c11\u65e0\u7a77\u7ea7\u6570Part1\uff08\u5f85\u6574\u7406\uff09</li> <li> 6.\u5b8c\u6210\u7ebf\u4ee3\u4f5c\u4e1a</li> <li> 7.\u4e00\u7bc7tpo</li> </ul>"},{"location":"diaries/2024/20241231/","title":"20241231","text":"<p> \u7ea6 5 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u6700\u540e\u4e00\u5929\u4e86</p>"},{"location":"diaries/2025/202050101/","title":"202050101","text":"<p> \u7ea6 36 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>NEW YEAR RESOLUTION</p> <p>1.\u65c5\u884c</p> <p>2.\u6444\u5f71</p> <p>3.\u8f6c\u4e13\u4e1a</p> <p>4.\u770b\u4e00\u573a\u6f14\u5531\u4f1a</p> <p>5.Be Happy</p> <p>\u65b0\u7684\u4e00\u5e74\u8fd8\u5f97\u7ee7\u7eed\u52aa\u529b</p> <p>\u7ed3\u675f2.1\u7684\u300a\u6df1\u5ea6\u5b66\u4e60\u300b</p>"},{"location":"diaries/2025/20250228/","title":"2\u6708","text":"<p> \u7ea6 63 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u4eca\u5929\u665a\u4e0a\u53bb\u5357\u4f53\u8dd1\u4e86\u4f1a\u6b65\u3002\u7531\u4e8e\u662f\u590d\u5065\u72b6\u6001\uff0c\u53ea\u8dd1\u4e862.6\u516c\u91cc\uff0c\u817f\u7279\u522b\u9178\u75db\u3002\u611f\u89c9\u4ee5\u540e\u8fd8\u662f\u5f97\u6bcf\u5929\uff08\uff1f\uff09\u953b\u70bc\u4e00\u4e0b\uff0c\u6bd5\u7adf\u73b0\u5728\u597d\u4e45\u90fd\u6ca1\u6253\u7fbd\u6bdb\u7403\u4e86\uff0c\u8be5\u6362\u79cd\u8fd0\u52a8\u65b9\u5f0f\u4e86\u3002</p>"},{"location":"notes/","title":"index","text":""},{"location":"notes/#notes","title":"Notes \ud83d\udcda","text":"<p>Abstract</p> <p>\u4e00\u4e9b\u6bd4\u8f83\u6210\u4f53\u7cfb\u7684\u7b14\u8bb0\u90fd\u505a\u5728\u8fd9\u91cc\uff0c\u65b9\u4fbf\u67e5\u9605\u3002</p> <p>\u672c\u90e8\u5206\u5185\u5bb9\uff08\u9664\u7279\u522b\u58f0\u660e\u5916\uff09\u91c7\u7528 \u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u4fdd\u6301\u4e00\u81f4 4.0 \u56fd\u9645 (CC BY-NC-SA 4.0) \u8bb8\u53ef\u534f\u8bae\u8fdb\u884c\u8bb8\u53ef\u3002</p>"},{"location":"notes/EECS498/A1/","title":"\u7b2c\u4e00\u6b21\u4f5c\u4e1a(pytorch & KNN)","text":"<p> \u7ea6 167 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>Deep Learning \\(\\in\\) Machine Learning</p> <p>Deep  Learning \\(\\cap\\) Computer Vision = EECS 498</p> <p>SIFT:\u8bc6\u522b\u56fe\u7247\u4e2d\u5c0f\u7684\u5339\u914d\u70b9\uff0c\u8ba1\u7b97\u7279\u5f81\u5411\u91cf\uff08\u4e0d\u4f1a\u56e0\u4e3a\u65cb\u8f6c\u4ee5\u53ca\u56fe\u7247\u7684\u4eae\u5ea6\u6539\u53d8\uff09</p> <p>\u79bb\u73b0\u5b9e\u751f\u6d3b\u8fd8\u6709\u4e00\u5b9a\u8ddd\u79bb\uff1a\u8ba1\u7b97\u673a\u53ea\u80fd\u770b\u5230\uff08\u8bc6\u522b\uff09\u4fe1\u606f\uff0c\u5374\u4e0d\u80fd\u7406\u89e3\u56fe\u7247\u80cc\u540e\u7684\u542b\u4e49\u3002\u4eba\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u4fe1\u606f\u6765\u63a8\u7406\u5224\u65ad\uff0c\u4f46\u8ba1\u7b97\u673a\u5f88\u96be\u505a\u5230\u3002</p> <p>x.min(dim=0)\uff1a\u4f1a\u8fd4\u56de\u4e24\u4e2a\u503c\uff0c\u7b2c\u4e00\u4e2a\u5c31\u662f\u6700\u5c0f\u503c\uff0c\u7b2c\u4e8c\u4e2a\u662f\u6700\u5c0f\u503c\u7684\u4f4d\u7f6e\uff08\u7d22\u5f15\uff09</p> <p>\u5982\u679c\u53ea\u60f3\u83b7\u5f97\u7d22\u5f15\u7684\u8bdd\uff0c\u5c31\u7528x.argmin()</p> <p>\u4e0b\u8f7dpytorch\u6559\u7a0b\u6587\u4ef6</p> <p>\u4e0b\u8f7dKNN\u7b97\u6cd5\u6587\u4ef6</p> <p>\u67e5\u770b Pytorch \u4ea4\u4e92 Notebook</p> <p>\u67e5\u770b KNN \u4ea4\u4e92 Notebook</p>"},{"location":"notes/EECS498/Back%20Propagation/","title":"\u7b2c\u56db\u8282(Back Propagation)","text":"<p> \u7ea6 474 \u4e2a\u5b57  18 \u884c\u4ee3\u7801  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 3 \u5206\u949f</p> <p>Back Propagation:</p> <p>Simple Example:\\(f(x,y,z)=(x+y)z\\)</p> <p>1.Forward Pass: Compute outputs from left to right(from inputs to outputs)</p> <p>\\(q=x+y\\) \\(f=qz\\)</p> <p>2.Backward Pass: Compute derivatives </p> <p>Want: \\(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}\\)</p> <p>Order: \\(\\frac{\\partial f}{\\partial f}\\rightarrow \\frac{\\partial f}{\\partial z}\\rightarrow \\frac{\\partial f}{\\partial q}\\rightarrow \\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}\\rightarrow \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\\)</p> <p>[Downstream] = [Local]*[Upstream]</p> <p>sigmoid function:\\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\)</p> <p>\\(\\frac{d\\sigma(x)}{dx}=(1-\\sigma(x))\\sigma(x)\\)</p> <p>add: don\u2019t change derivatives so the elements linked by the \u201c+\u201d share the same gradient.</p> <p>copy: one side\u2019s derivatives add to the other side.</p> <p>multiply: swap multiplier</p> <p>max: gradient router,that reduces one element to \\(0\\) and another,full gradient the same with other side\u2019s element\u2019s.</p> <p>Flat: \u201cReverse Thinking Method\u201d:</p> Python<pre><code>#Forward\ndef f(w0,x0,w1,x1,w2):\n    s0 = w0 * x0\n    s1 = w1 * x1\n    s2 = s0 + s1\n    s3 = s2+ w2\n    L = sigmoid(s3)\n#Backward\n    grad_L = 1.0\n    grad_s3 = grad_L * (1-L) * L    #sigmoid function has a special form of derivatives\n    grad_w2 = grad_s3   #gradient copier\n    grad_s2 = grad_s3   #gradient copier\n    grad_s0 = grad_s2   #gradient copier\n    grad_s1 = grad_s2   #gradient copier\n    grad_w1 = grad_s1 * x1  #gradient multiplier\n    grad_x1 = grad_s1 * w1  #gradient multiplier\n    grad_w0 = grad_s0 * x0  #gradient multiplier\n    grad_x0 = grad_w0 * w0  #gradient multiplier\n</code></pre> <p>\\(y \\in \\mathbb{R},x \\in \\mathbb{R}^M,\\frac{dy}{dx}\\in \\mathbb{R}^M\\)</p> <p>\\(y \\in \\mathbb{R}^N,x \\in \\mathbb{R}^M,\\frac{dy}{dx}\\in \\mathbb{R}^{M\\times N}\\) :Jacobian Matrix</p> <p>4D INPUT \\(x\\):[1,-2,3,-3]  $\\rightarrow f(x)=\\max(0,x)(elementwise)\\rightarrow $  4D OUTPUT \\(y\\) = [1,0,3,0]</p> <p>4D \\(\\frac{dL}{dy}\\):[4,-1,5,9] $\\rightarrow $ \\(\\frac{dy}{dx}\\frac{dL}{dy}\\)</p> <p>\\(\\frac{dy}{dx}\\)=\\(\\(\\begin{bmatrix}1&amp;0&amp;0&amp;0 \\\\0&amp;0&amp;0&amp;0 \\\\0&amp;0&amp;1&amp;0\\\\0&amp;0&amp;0&amp;0  \\end{bmatrix}\\)\\)positive: 1 negative: 0</p> <p>Jacobian is sparse!: off-diagonal entries all zero! When doing a big Jacobian Matrix Multiply,it\u2019ll cause large resources waste because almost every element is zero!Never explicitly form Jacobian,instead use implicit multiplication.</p> <p>\\(y=xw\\)(\\(x:[N\\times D]\\) \\(w:[D\\times M]\\) \\(y:[N \\times M]\\))</p> <p>\\(\\frac{dL}{dx_{i,j}}=\\frac{dy}{dx_{i,j}}\\cdot\\frac{dL}{dy}=w_{j,:}\\cdot\\frac{dL}{dy_{i,:}}\\)</p> <p>\\(\\frac{dL}{dx}=\\frac{dL}{dy}w^T\\)(How to remember? Use the shape!)</p> <p>\\(\\frac{dL}{dw}=x^T\\frac{dL}{dy}\\)</p> <p>Hint:     \\(\\cdot\\)  means inner product while blank space means matrix multiply</p> <p>Backward-Mode: A vector input and a scalar output</p> <p>Forward-Mode: A scalar input and a vector output</p> <p>Compute Higher-Order Derivatives(Cool!):</p> <p>\\(x_0 --f1--&gt;x1--f2--&gt;L--f_2'--&gt;\\frac{dL}{dx_1}--f_1'--&gt;\\frac{dL}{dx_0}--\\cdot v--&gt;\\frac{dL}{dx_0}\\cdot v\\)</p> <p>we want to calculate \\(\\frac{\\partial^2L}{\\partial x_0^2}\\)  then we can calculate\\(\\frac{\\partial^2L}{\\partial x_0^2}\\cdot v\\) ,  and surprisingly,\\(\\frac{\\partial^2L}{\\partial x_0^2}\\cdot v=\\frac{\\partial}{\\partial x_0}[\\frac{\\partial L}{\\partial x_0}\\cdot v]\\)</p> <p>(\\(v\\) is independent from \\(x_0\\)) </p> <p>use backprop we will get the answer(remember: backprop gets the derivatives of output with regard to the input)</p>"},{"location":"notes/EECS498/Linear%20Classifiers/","title":"\u7b2c\u4e00\u8282(Linear Classifiers)","text":"<p>Linear Classification: \\(f(x_i,W) = W \\cdot x\\)</p> <p>Matrix multiply: stretch x to a one-dimension vector,W is a matrix.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/#multiclass-svm-loss","title":"Multiclass SVM Loss:","text":"<p> \u7ea6 366 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> <p>Let \\(f(x_i,W)\\) be scores,then the SVM scores has the form: \\(L_i = \\sum_{j\\neq y_i}\\max(0,s_j-s_{y_i}+1)\\)</p> <p>\\(s_{y_i}\\) is the correct label\u2019s score,while \\(s_j\\) is the wrong label\u2019s scores. When \\(s_j\\) is larger than \\(s_{y_i} - 1\\)</p> <p>,that means it contributes to the loss,so that \\(L_i\\) is greater than \\(0\\).</p> <p>Characteristics: 1.When give the \\(s_{y_i}\\) a little bit change,the Loss function will not change. Because after change,\\(s_{y_i}\\) is still 1 more than the wrong label\u2019s scores.</p> <p>min possible : 0 max:\\(+\\infty\\)</p> <p>When all scores are small random values,loss is \\(C - 1\\)(\\(s_j \\approx s_{y_i}\\)) where C stands for the number of categories.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/#regularization","title":"Regularization","text":"<p>\\(L(W)=\\frac{1}{N}\\sum_{i=1}^NL_i(f(x_i,W),y_i)+\\lambda R(W)\\) </p> <p>The most common regularization: L2-norm \\(\\sum_i\\sum_jW_{i,j}^2\\) </p> <p>Why we need that?:</p> <ul> <li> <p>Express preferences in among models beyond \u201cminimize training error\u201d,allow people to integrate their wisdom and knowledge they\u2019ve already obtained.</p> </li> <li> <p>Avoid overfitting </p> </li> </ul> <p>Example: \\(x = [1,1,1,1] \\newline w_1=[1,0,0,0] \\newline w_2=[0.25,0.25,0.25,0.25]\\)</p> <p>It\u2019s obvious that \\(w_1^\\mathrm T \\cdot x = w_2^\\mathrm T\\cdot x = 1\\)</p> <p>L2-norm regularization prefer more balanced matrix,which is \\(w_2\\) in this example. This implies that use as many functions as possible in this preference.\u201dspread out the weights\u201d</p> <p>prefer simple models: Occam's Razor reveals the truth that simplicity is much preferred.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/#cross-entropy-loss","title":"Cross Entropy Loss","text":"<p>SoftMax function: </p> cat 3.2 24.5 0.13 car 5.1 164.0 0.87 frog -1.7 0.18 0.00 <p>\u200b               unnormalized log-prob/logits --exp\u2192 unnormalized prob --normalize\u2192probabilities</p> <p>\\(L_i = -\\ln P(Y = y_i |X = x_i)\\)  Maximum Likelihood Estimation</p> <p>min possible loss:0 (it can only approach to 0 but never truly reach)   max:\\(+\\infty\\)</p> <p>When all scores are small random values,loss is \\(-\\ln C\\) where C stands for the number of categories.</p>"},{"location":"notes/EECS498/Neural%20Network/","title":"\u7b2c\u4e09\u8282(Neural Network)","text":"<p> \u7ea6 319 \u4e2a\u5b57  16 \u884c\u4ee3\u7801  5 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> <p>Linear Classifiers cannot deal with non-linear boundaries.</p> <p>such as two circles with different radius but the same center point.</p> <p>\u2192 use polar coordinate to create a feature space. \\(r\\) is the x axis and the \\(\\theta\\) is the y axis.</p> <p>all points on the same circle is seated on the same vertical line that is parallel to the y axis.</p> <p>(Before) Linear score function: only a small part of Feature Extraction can adjust itself to better maximizing its ability.</p> <p>Learn only one template of one category.</p> <p>(After) Neural Network: raw picture pixel \u2192 classification scores</p> <p>Learn several templates of one category.</p> <p>Linear score function: \\(f = Wx\\)</p> <p>2-layer Neural Network:\\(f=W_2\\max(0,W_1x)\\)</p> <p>\u200b               \\(W_2 \\in \\mathbb{R}^{C\\times H} \\:  W_1 \\in \\mathbb{R}^{H\\times D}\\:  x \\in \\mathbb{R}^D\\)</p> <p>\\(h = W_1x = (\\alpha_1 ,\\alpha_2,\\cdots,\\alpha_H)^Tx\\)</p> <p>Element \\((i,j)\\) of \\(W_1\\) gives the effect on \\(h_i\\) from \\(x_j\\)</p> <p>Deep Neural Networks: Depth = number of layers = number of Matrix</p> <p>\u200b   Width = Size of each layer</p> <p>Activation Functions:</p> <p>Without the activation function,we will go back to \\(f=W_2W_1x=Wx\\) which is linear classifiers.</p> Activation Functions Expression Graph Sigmoid \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\) tanh tanh(x) ReLU(A good default choice for most problems) max(0,x) <p>A simple achievement:</p> Python<pre><code>import numpy as np\nfrom numpy.random import randn\n\nN,Din,H,Dout = 64,1000,100,10\nx,y = randn(N,Din),randn(N,Dout)\nw1,w2 = randn(Din,H),randn(H,Dout)\nfor t in range(10000):\n    h = 1.0 / (1.0 + np.exp(-x.dot(w1)))\n    y_pred = h.dot(w2)\n    loss = np.square(y_pred - y).sum()\n    dy_pred = 2.0 * (y_pred - y)\n    dw2 = h.T.dot(dy_pred)\n    dh = dy_pred.dot(w2.T)\n    dw1 = x.T.dot(dh*h*(1-h))\n    w1 -= 1e-4 * dw1\n    w2 -= 1e-4 * dw2\n</code></pre> <p>Space warping:</p> <p>Linear transform cannot linearly separate points even in feature space.</p> <p>but with ReLU function,</p> <p>Universal Approximation:</p> <p>\u200b   use layer bias to move the graph</p> <p></p> <p>use many ReLU to approach the function.</p> <p>to reach 0 or unchanged: slope should be opposite</p> <p>let coefficient of x be 1,only change the shaping factor of MAX.</p> <p>Convex Functions:</p> <p>\\(f:X \\subset \\mathbb{R}^N \\rightarrow \\mathbb{R}\\) is convex if for all \\(x_1,x_2 \\in X,t\\in[0,1],f(tx_1+(1-t)x_2)\\leq tf(x_1)+(1-t)f(x_2)\\)</p> <p>convex is easy to optimize</p>"},{"location":"notes/EECS498/Optimization/","title":"\u7b2c\u4e8c\u8282(Optimization)","text":"<p>\\(w^* = \\arg \\min_wL(w)\\)</p> <p>Idea #1 :Random Search(Bad Idea!)</p> Python<pre><code>bestloss = float('inf')\nfor num in xrange(1000):\n    W = np.random.randn(10,3073) * 0.001\n    loss = L(X_train,Y_train,W) #L is the loss function\n    if loss &lt; bestloss:\n        bestloss = loss\n        bestW = W\n    print(f'in attempt {num} the loss was {loss},best {bestloss}')\n</code></pre> <p>Batch Gradient Descent</p> <p>\\(L(W) = \\frac{1}{N}\\sum_{i=1}^NL_i(x_i,y_i,W)+\\lambda R(W)\\)</p> <p>\\(\\nabla_WL(W)=\\frac{1}{N}\\sum_{i=1}^N\\nabla_WL_i(x_i,y_i,W)+\\lambda\\nabla_WR(W)\\)</p> <p>Idea #2 : Stochastic Gradient Descent</p> Python<pre><code>w = initialize_weights()\nfor t in range(num_steps):\n    minibatch = sample_data(data,batch_size)\n    dw = compute_gradient(loss_fn,minibatch,w)\n    w- = learning_rate * dw\n</code></pre> <p>SGD: \\(x_{t+1}=x_t - \\alpha \\nabla f(x_t)\\)</p> <p>Problems:1.overshoot and never get back</p> <p>\u200b       2.settle in local minimum and saddle point</p> <p>SGD+Momentum: \\(v_{t+1}=\\rho v_t -\\alpha \\nabla f (x_t)\\)</p> <p>\u200b               \\(x_{t+1}=x_t+v_{t+1}\\) </p> <p>Nesterov Momentum:\\(v_{t+1}=\\rho v_t-\\alpha \\nabla f(x_t+\\rho v_t)\\)</p> <p>\u200b               \\(x_{t+1}=x_t+v_{t+1}\\)   Not that good :Not intuitively clear,because it uses the data of future status</p> <p>\u200b       or            $\\tilde{x_t} =x_t + \\rho v_t $</p> <p>\u200b               \\(v_{t+1}=\\rho v_t -\\alpha \\nabla f (\\tilde{x_t})\\)</p> <p>\u200b               \\(\\widetilde{x_{t+1}}=\\tilde{x_t}-\\rho v_t + (1 + \\rho) v_{t+1}\\)</p> <p>\u200b                   \\(=\\tilde{x_t}+v_{t+1}+\\rho (v_{t+1}-v_t)\\)</p> <p>AdaGrad: Progress along \u201csteep\u201d directions is damped;</p> <p>\u200b       progress along \u201cflat\u201d directions is accelerated.</p> Python<pre><code>grad_squared = 0\nfor t in range(num_steps):\n    dw = compute_gradient(w)\n    grad_squared += dw*dw\n    w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)\n</code></pre> <p>Problem: grad_squared is accumulative so that it will stop before getting to the bottom.(it can get very big)</p> <p>RMSProp: a leaky version of Adaguard</p> Python<pre><code>grad_square = 0\nfor t in range(num_steps):\n    dw = compute_gradient(w)\n    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dw * dw\n    w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)\n</code></pre> <p>Adam: RMSProp + Momentum</p> Python<pre><code>moment1 = 0\nmoment2 = 0\nfor t in range(num_steps):\n    dw = compute_gradient(w)\n    moment1 = beta1 * moment1 + (1-beta1) * dw #Momentum\n    moment2 = beta2 * moment2 + (1-beta2) * dw * dw #RMSProp\n    #moment1_unbias = moment1 / (1 - beta1 ** t)\n    #moment2_unbias = moment2 / (1 - beta2 ** t)\n    w -= learning_rate * moment1 / (moment2.sqrt() + 1e-7)\n    # Problem: when beta2 is approximately 1,momenent.sqrt() in the first several steps can be very small,thus leading to the */moment2.sqrt() very big.\n&lt;div markdown=\"1\" style=\"margin-top: -30px; font-size: 0.75em; opacity: 0.7;\"&gt;\n:material-circle-edit-outline: \u7ea6 234 \u4e2a\u5b57 :fontawesome-solid-code: 34 \u884c\u4ee3\u7801 :material-clock-time-two-outline: \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f\n&lt;/div&gt;\n    #We need to correct the bias.  \n</code></pre> <p>Adam with beta1 = 0.9,beta2 = 0.999,and learning_rate = 1e-3,5e-4,1e-4 is a great starting point for many models.</p>"},{"location":"summary/","title":"index","text":""},{"location":"summary/#summaries","title":"Summaries \ud83d\uddd3\ufe0f","text":"\u300e \u0915\u093f\u0928\u094d\u0928\u0930\u093f\u092f \u092e\u092e \u0924\u0923\u094d\u0939\u093e \u300f"},{"location":"summary/%E5%85%B7%E4%BD%93%E8%AE%A1%E5%88%92/","title":"\u89c4\u5212","text":"<p> \u7ea6 46 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u6570\u5b66\u5206\u6790\uff1a\u8ddf\u7740\u8001\u5e08\u4e0a\u8bfe\u8d70 \u7ed3\u5408\u8c22\u60e0\u6c11\u8003\u524d\u7a81\u51fb</p> <p>\uff08\u91cd\u8981\uff09\u7a0b\u5e8f\u8bbe\u8ba1\uff1a\u81ea\u5b66 ACMOJ CS61B</p> <p>\u5927\u5b66\u7269\u7406\uff1a\u81ea\u5b66 </p> <p>\uff08\u91cd\u8981\uff09\u6982\u7387\u7edf\u8ba1\uff1aIntroduction to Probability MIT6.041</p>"},{"location":"summary/%E6%88%91%E7%9A%84%E5%A4%A7%E4%B8%80%E4%B8%8A%E5%AD%A6%E6%9C%9F/","title":"\u590d\u76d8","text":"<p> \u7ea6 9 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p>"},{"location":"summary/%E6%88%91%E7%9A%84%E5%A4%A7%E4%B8%80%E4%B8%8A%E5%AD%A6%E6%9C%9F/#_1","title":"\u603b\u7ed3\u6211\u7684\u5927\u4e00\u4e0a\u5b66\u671f","text":""}]}