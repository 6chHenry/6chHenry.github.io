{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u4e3b\u9875","text":"Welcome to 6ch.'s Site! \ud83c\udf89  <p>  About Me /   Academic Page /  Statistics </p> <li>Website Operating Time: </li> <li>Total Visitors:  people</li> <li>Total Visits:  times</li>"},{"location":"academy/","title":"KeWei Liu(\u5218\u53ef\u552f)","text":""},{"location":"academy/#kewei-liu","title":"KeWei Liu(\u5218\u53ef\u552f)","text":"<p> Work Email: HenryLiu</p> <p> Personal Email: 2313287840 [at] qq [dot] com</p> <p> CV: Empty</p> <p></p>"},{"location":"academy/#bio","title":"Bio","text":"<p>I am a first-year undergraduate student majoring in Mechanics at the ME of Shanghai Jiao Tong University (SJTU). Currently, I am an active participant in Zhiyuan Honors Program, which fosters academic excellence and innovation.</p> <p>I am actively seeking internships in the field of machine learning systems and optimization. Please feel free to reach out to me if you have any opportunities available! \ud83e\udd70\ud83e\udd70\ud83e\udd70</p>"},{"location":"academy/#research-interest","title":"Research Interest","text":"<ul> <li>Machine Learning Systems for Advancing AI and AGI: I am committed to the development of robust and efficient machine learning systems, recognizing that the advancement of AI and AGI is fundamentally constrained by the capabilities of their underlying system infrastructures.</li> <li>Optimization in Large Model Training and Reasoning: I also focus on improving the efficiency and practicality of large-scale model training and inference through advanced algorithms.</li> </ul>"},{"location":"academy/#news","title":"News","text":"20252024 <p>[02/2025] I became a member of Zhiyuan Honor Program,SJTU.</p> <p>[09/2024] Excited to start my ME B.Eng. at SJTU ME.</p>"},{"location":"academy/#education","title":"Education","text":""},{"location":"academy/#zhiyuan-honor-program-shanghai-jiao-tong-university","title":"Zhiyuan Honor Program, Shanghai Jiao Tong University","text":"<p>Feb. 2025 -- Present</p> <p></p>"},{"location":"academy/#school-of-mechanical-engineering-shanghai-jiao-tong-university","title":"School of Mechanical Engineering, Shanghai Jiao Tong University","text":"<p>Sept. 2024 -- Present</p>"},{"location":"academy/#publications-manuscripts","title":"Publications &amp; Manuscripts","text":"<p>Coming soon...</p>"},{"location":"academy/#experience","title":"Experience","text":""},{"location":"academy/#projects","title":"Projects","text":"<p>The page is powered by Material for MkDocs and supports collaboration through Pull Requests and GitHub Actions.</p>"},{"location":"academy/#media-exposures","title":"Media Exposures","text":""},{"location":"academy/#honors","title":"Honors","text":""},{"location":"about/","title":"\u5173\u4e8e\u6211","text":""},{"location":"about/#about","title":"About \ud83e\udd73","text":"<p> \u7ea6 409 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> \u4e2d\u6587English <p>\u4f60\u597d\uff01\u6211\u662f 6ch.\uff0c\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 24 \u7ea7\u673a\u68b0\u7c7b\u4e13\u4e1a\u7684\u672c\u79d1\u751f\u3002\u5f88\u9ad8\u5174\u80fd\u5728\u4e92\u8054\u7f51\u4e0a\u4e0e\u4f60\u76f8\u9047\ud83e\udd70\u3002</p> <p>\u6211\u5bf9\u6df1\u5ea6\u5b66\u4e60\u3001\u5927\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7b49\u9886\u57df\u5f88\u6709\u70ed\u60c5\uff0c\u4e5f\u559c\u6b22\u5206\u4eab\u81ea\u5df1\u7684\u5b66\u4e60\u7ecf\u9a8c\u548c\u601d\u8003\ud83e\udd13\u3002</p> <p>\u60f3\u4e86\u89e3\u6211\u7684\u5b66\u672f\u7ecf\u5386\uff1f\u53ef\u4ee5\u67e5\u770b\u6211\u7684\u5b66\u672f\u4e3b\u9875\ud83c\udf93\u3002</p> <p>\u5982\u679c\u6709\u4efb\u4f55\u95ee\u9898\uff0c\u6216\u8005\u60f3\u548c\u6211\u4ea4\u6d41\uff0c\u6b22\u8fce\u53d1\u90ae\u4ef6\u6216\u8005\u76f4\u63a5\u5728\u4e0b\u65b9\u7559\u8a00\uff0c\u6211\u4f1a\u5c3d\u5feb\u56de\u590d\ud83d\ude0e\u3002  \u5982\u679c\u4f60\u4e5f\u5728\u4e0a\u6d77\uff0c\u6b22\u8fce\u8054\u7cfb\u6211\u3002\u6211\u4eec\u53ef\u4ee5\u4e00\u8d77\u804a\u5929\u3001\u5b66\u4e60\uff0c\u6216\u8005\u4ea4\u6d41\u5171\u540c\u7684\u5174\u8da3\u7231\u597d\ud83d\udc7b\u3002</p> <p>GitHub \u70b9\u4e2a\u5173\u6ce8\u8c22\u8c22\u55b5\ud83d\ude3a\uff0cGitHub \u70b9\u4e2a\u5173\u6ce8\u8c22\u8c22\u55b5\ud83d\ude3a</p> <p>\u6211\u5e73\u65f6\u559c\u6b22\u542c\u97f3\u4e50(Eason!)\u3001\u9605\u8bfb\u3001\u6253\u7fbd\u6bdb\u7403\uff0c\u4e5f\u4f1a\u5728\u65e5\u8bb0\u4e2d\u8bb0\u5f55\u4e00\u4e9b\u76f8\u5173\u5185\u5bb9\uff0c\u5206\u4eab\u4e2a\u4eba\u611f\u53d7\u270d\u3002</p> <p>\u81f3\u4e8e\u6211\u7684\u540d\u5b57\u201c6ch.\u201d\u561b\uff0c\u89e3\u91ca\u8d77\u6765\u592a\u56f0\u96be\u4e86\uff0c\u5c31\u5f53\u662f\u6211\u968f\u4fbf\u53d6\u7684\u597d\u5566\uff01</p> <p>Hello. I'm 6ch! I'm 6ch. and I'm an undergraduate student majoring in Mechanics in the 24<sup>th</sup> grade at Shanghai Jiaotong University. I'm glad to meet you on the internet \ud83e\udd70.</p> <p>I am passionate about the fields of deep learning, big models and machine learning systems, and I like to share my learning experience and thinking \ud83e\udd13.</p> <p>Want to know more about my academic experience? Check out my [academic homepage \ud83c\udf93] (.. /academy.md).</p> <p>If you have any questions or would like to communicate with me, please feel free to email or just leave a message below and I will reply as soon as possible \ud83d\ude0e. If you are also in Shanghai, feel free to contact me. We can chat, learn, or exchange common interests together \ud83d\udc7b.</p> <p>Follow my github thank you meow \ud83d\ude3a, follow my Github thank you meow \ud83d\ude3a~</p> <p>I usually like listening to music(Eason!), reading and playing badminton, and I also write diaries about it and share my personal feelings \u270d.</p> <p>As for my name \u201c6ch.\u201d, it's a long long story.</p>"},{"location":"diaries/","title":"index","text":""},{"location":"diaries/#diaries","title":"Diaries \u270d","text":"<p>Abstract</p> <p>\u4e2a\u4eba\u65e5\u8bb0\uff0c\u4e3b\u8981\u8bb0\u5f55</p> <ul> <li>\u751f\u6d3b\u611f\u609f(Just Be Happy!)\uff1b</li> <li>\u8bfb\u4e66\u6458\u5f55\uff0c\u53ef\u80fd\u4f1a\u6709\u4e00\u4e9b\u7b14\u8bb0\uff1b</li> <li>\u4e00\u4e9b\u6742\u8c08\u3002</li> </ul> <p>\u4e00\u4e9b\u6bd4\u8f83\u6210\u4f53\u7cfb\u7684\u7b14\u8bb0\u4f1a\u8bb0\u5f55\u5728 Notes \u4e2d\u3002</p> <p>\u672c\u90e8\u5206\u5185\u5bb9\uff08\u9664\u7279\u522b\u58f0\u660e\u5916\uff09\u91c7\u7528 \u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u4fdd\u6301\u4e00\u81f4 4.0 \u56fd\u9645 (CC BY-NC-SA 4.0) \u8bb8\u53ef\u534f\u8bae\u8fdb\u884c\u8bb8\u53ef\u3002</p>"},{"location":"diaries/#archives","title":"Archives","text":"<p>\u5982\u679c\u5bfb\u627e\u4e0d\u65b9\u4fbf\u7684\u8bdd\uff0c\u4e0d\u59a8\u8bd5\u8bd5\u641c\u7d22</p>"},{"location":"diaries/2024/20241226/","title":"20241226","text":"<p> \u7ea6 58 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u4e3b\u8981\u4efb\u52a1\uff1a</p> <ul> <li> 1.\u68b3\u7406\u671f\u672b\u8003\u8bd5\u590d\u4e60\u8ba1\u5212  </li> <li> 2.\u68b3\u7406\u5237\u8bfe\u8fdb\u5ea6\u5b89\u6392</li> <li> 3.\u5b8c\u6210\u6570\u5206\u4f5c\u4e1a</li> <li> 4.\u590d\u4e60\u82f1\u8bedquiz</li> <li> 5.\u505a\u8c22\u60e0\u6c11\u65e0\u7a77\u7ea7\u6570Part1\uff08\u5f85\u6574\u7406\uff09</li> <li> 6.\u5b8c\u6210\u7ebf\u4ee3\u4f5c\u4e1a</li> <li> 7.\u4e00\u7bc7tpo</li> </ul>"},{"location":"diaries/2024/20241231/","title":"20241231","text":"<p> \u7ea6 5 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u6700\u540e\u4e00\u5929\u4e86</p>"},{"location":"diaries/2025/20250101/","title":"1\u6708","text":"<p> \u7ea6 36 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>NEW YEAR RESOLUTION</p> <p>1.\u65c5\u884c</p> <p>2.\u6444\u5f71</p> <p>3.\u8f6c\u4e13\u4e1a</p> <p>4.\u770b\u4e00\u573a\u6f14\u5531\u4f1a</p> <p>5.Be Happy</p> <p>\u65b0\u7684\u4e00\u5e74\u8fd8\u5f97\u7ee7\u7eed\u52aa\u529b</p> <p>\u7ed3\u675f2.1\u7684\u300a\u6df1\u5ea6\u5b66\u4e60\u300b</p>"},{"location":"diaries/2025/20250228/","title":"2\u6708","text":"<p> \u7ea6 63 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u4eca\u5929\u665a\u4e0a\u53bb\u5357\u4f53\u8dd1\u4e86\u4f1a\u6b65\u3002\u7531\u4e8e\u662f\u590d\u5065\u72b6\u6001\uff0c\u53ea\u8dd1\u4e862.6\u516c\u91cc\uff0c\u817f\u7279\u522b\u9178\u75db\u3002\u611f\u89c9\u4ee5\u540e\u8fd8\u662f\u5f97\u6bcf\u5929\uff08\uff1f\uff09\u953b\u70bc\u4e00\u4e0b\uff0c\u6bd5\u7adf\u73b0\u5728\u597d\u4e45\u90fd\u6ca1\u6253\u7fbd\u6bdb\u7403\u4e86\uff0c\u8be5\u6362\u79cd\u8fd0\u52a8\u65b9\u5f0f\u4e86\u3002</p>"},{"location":"diaries/2025/20250301/","title":"3.1","text":"<p> \u7ea6 95 \u4e2a\u5b57  1 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u4eca\u5929\u82b1\u4e86\u597d\u4e45\u7ec8\u4e8e\u628a\u7f51\u7ad9\u642d\u597d\u4e86\u3002\u6700\u7ec8\u91c7\u7528Actions\u89e3\u51b3\u7684\uff0c\u7b49\u6709\u7a7a\u4e86\u5199\u4e2a\u7ecf\u9a8c\u5e16\u5b50\u3002</p> <p>\u4e0a\u5348\u53bb\u62cd\u4e86\u81f4\u8fdc\u65b0\u751f\u7167\uff0c\u611f\u89c9\u81ea\u5df1\u633a\u5e05\u7684\uff0c\u81f3\u5c11\u5f88\u9633\u5149\u3002\u8fd8\u6709\u5c31\u662f\u6700\u8fd1\u53c8\u7626\u4e86\u4e00\u70b9\uff0c\u53ef\u80fd\u8981\u66f4\u7cbe\u795e\u4e00\u4e9b\u3002</p> <p>\u4eca\u5929EECS\u5b66\u5230\u4e86\u7b2c\u516d\u8bfe\uff0c\u660e\u5929\u628aassignment02\u7ed9\u5199\u6389\u3002</p> <p>\u9644\u4e0a\u4eca\u65e5\u6253\u5361\uff1a</p>"},{"location":"diaries/2025/20250302/","title":"3.2","text":"<p> \u7ea6 405 \u4e2a\u5b57  1 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> Text Only<pre><code>\u4eca\u5929\u82b1\u4e86\u51e0\u4e4e\u4e00\u6574\u5929\u7684\u65f6\u95f4\u5b8c\u6210EECS498-A2\uff0c\u5e76\u4e14\u53ea\u662fA2\u7684\u4e00\u90e8\u5206(Linear Classifier)\u3002\u771f\u7684\u597d\u96be\u3002\u77e9\u9635\u6c42\u5bfc\u3001\u94fe\u5f0f\u6cd5\u5219\u3001\u5404\u79cd\u5de7\u5999\u7684\u53d8\u5f62\uff0c\u8d8a\u53d1\u89c9\u5f97\u5b66\u597d\u7ebf\u6027\u4ee3\u6570\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u6570\u5b66\u5728\u5176\u4e2d\u626e\u6f14\u7684\u91cd\u8981\u89d2\u8272\u3002\u5f88\u591a\u6280\u5de7\u4e5f\u53ef\u4ee5\u7528\u5230\u6570\u5b66\u5f53\u4e2d\u3002\u5f88\u591a\u65f6\u5019\u90fd\u641e\u4e0d\u592a\u61c2\u77e9\u9635\u7684\u5f62\u72b6\uff0c\u4e0d\u77e5\u9053\u600e\u4e48\u8fd0\u7b97\u624d\u662f\u6b63\u786e\u7684\u3002\u54ce~\u6162\u6162\u6765\u5427\n</code></pre> <p>\u200b   \u4e0b\u5348\u53bbA+\u4ff1\u4e50\u90e8\u9762\u8bd5\u4e86\uff0c\u611f\u89c9\u5c1a\u53ef\uff0c\u4ecb\u7ecd\u7684\u65f6\u5019\u8fd8\u662f\u6709\u70b9\u7d27\u5f20\u3002\u8bf4\u5b9e\u8bdd\u4e0d\u77e5\u9053\u81ea\u5df1\u5728\u7126\u8651\u4ec0\u4e48\uff0c\u660e\u660e\u8fd9\u4e2a\u662f<sub>~\u5341\u62ff\u4e5d\u7a33\u7684\u4e8b\u60c5</sub>~\u3002\u4e0d\u8fc7\u4e5f\u7b97\u662f\u4e86\u5374\u4e86\u81ea\u5df1\u7684\u5206\u4eab\u6b32\u548c\u5c55\u793a\u6b32\u5427\u3002</p> <p>\u200b   \u665a\u4e0a\u8fd8\u6709\u5f88\u591a\u4e8b\u60c5\uff0c\u611f\u89c9\u6bcf\u5929\u90fd\u5728\u52aa\u529b\u5b66\u4e60\uff0c\u4e0d\u8fc7\u611f\u89c9\u6ca1\u6709\u771f\u6b63\u641e\u61c2\uff0c\u6548\u76ca\u4e0d\u5927\u3002\u4e0b\u5468\u53ef\u4ee5\u9002\u5f53\u8c03\u6574\u4e00\u4e0b\uff0c\u6bd4\u5982\u628a\u76f8\u5173\u7684\u77e5\u8bc6\u638c\u63e1\u7262\u9760\u518d\u7ee7\u7eed\u5b66\uff0ci.e.\u77e9\u9635\u6c42\u5bfc\uff0c\u53cd\u5411\u4f20\u64ad\u3002\u6211\u559c\u6b22\u62ffcsdiy\u91cc\u7684\u4e00\u53e5\u8bdd\u6fc0\u52b1\u81ea\u5df1\uff1a\u201c\u53ea\u6709\u4e00\u4e2a\u5ff5\u5934\uff0c\u5c31\u662f\u4f60\u6b63\u5728\u53d8\u5f3a\u3002\u201d</p> <p>\u200b   \u6700\u8fd1\u8bfe\u5185\u7684\u538b\u529b\u4e5f\u5f88\u5927\uff0c\u6570\u5206\u3001\u5927\u7269\u90fd\u5f00\u59cb\u4e0a\u96be\u5ea6\u4e86\uff0c\u5927\u7269\u771f\u7684\u597d\u96be\uff0c\u53ea\u80fd\u9760\u8003\u524d\u505a\u9898+\u62df\u5408\u4e86\u3002\u6982\u7edf\u7684\u8bdd\u4ee5\u540e\u8fd8\u662f\u4e0d\u53bb\u4e0a\u8bfe\u4e86\uff0c\u591a\u770b\u4e66\u5427\u3002</p> <p>\u200b   \u597d\u4e45\u6ca1\u53bb\u6253\u7403\u4e86\uff0c\u611f\u89c9\u8eab\u4f53\u771f\u7684\u8981\u751f\u9508\u4e86\u3002</p> <p>\u200b   \u4eca\u5929\u5929\u6c14\u5f88\u597d\uff0c\u9762\u8bd5\u7ed3\u675f\u4e4b\u540e\u53bb\u673a\u7535\u5927\u8349\u576a\u901b\u4e86\u4e00\u5708\uff0c\u8349\u576a\u4e0a\u957f\u6ee1\u4e86\u4eba\u3002\u597d\u591a\u4eba\u5728\u62cd\u7167\uff0c\u771f\u597d\u3002</p> <p>\u200b   \u8fd8\u662f\u7ee7\u7eed\u52aa\u529b\u5427\u3002</p> <p>\u8865\u4e00\u4e2a\u4eca\u5929\u7684\u5b66\u4e60\u65f6\u95f4\uff1a</p>"},{"location":"diaries/2025/20250303/","title":"3.3","text":"<p> \u7ea6 148 \u4e2a\u5b57  1 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>\u7ef7\u4e0d\u4f4f\u4e86\u4eca\u5929\u3002\u4eba\u600e\u4e48\u53ef\u4ee5\u8fd9\u4e48\u7b28\uff0c\u8fd9\u4e48\u8822\u3002\u3002\u3002</p> <p>\u6211\u4eca\u5929\u665a\u4e0a\u516d\u70b9\u4e0d\u5230\u5c31\u53bb\u7269\u7406\u5b9e\u9a8c\u5ba4\u4e86\uff0c\u7ed3\u679c\u5750\u4e86\u534a\u5929\u624d\u53d1\u73b0\u6211\u8be5\u662f\u4e0b\u5468\u505a\u3002</p> <p>\u4eca\u5929\u4e0a\u5348\u56fe\u4e66\u9986\u53c8\u8fdd\u7ea6\u4e86\uff0c\u6628\u5929\u665a\u4e0a\u62a2\u4e86\u4f4d\u7f6e\u5fd8\u8bb0\u6539\u65f6\u95f4\u4e86\u3002</p> <p>\u597d\u5728\u4eca\u5929\u6709\u4e2a\u597d\u6d88\u606f\u662f\u6211\u52a0\u5165A+\u4ff1\u4e50\u90e8\u4e86\uff0c\u53c8\u62a5\u540d\u53c2\u52a0\u4e86\u8363\u6636\u50a8\u624d\u8ba1\u5212\uff0c\u611f\u89c9\u8fd8\u4e0d\u9519\u3002</p> <p>\u4eca\u5929\u665a\u4e0a\u641e\u4e86\u4e09\u4e2a\u5c0f\u65f6\u7684A2\u7b14\u8bb0\uff0c\u8fd8\u662f\u6ca1\u6574\u61c2\uff0c\u6211\u771f\u7684\u670d\u4e86\u6211\u8fd9\u7b28\u8111\u5b50\u4e86\u3002\u3002\u3002</p> <p>\u9644\u4e0a\u4eca\u5929\u7684\u65f6\u95f4\u5b89\u6392\uff1a</p>"},{"location":"diaries/2025/20250304/","title":"3.4","text":"<p> \u7ea6 175 \u4e2a\u5b57  1 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>\u4eca\u5929\u4e0b\u5348\u53bb\u6e38\u6cf3\u4e86\uff0c\u597d\u4e45\u6e38\u6cf3\u4e86\uff0c\u524d\u9762\u8fd8\u5728\u6c34\u91cc\u6251\u817e\uff0c\u6cf3\u955c\u53c8\u6ca1\u5e26\u7d27\u3002\u4e0d\u8fc7\u57fa\u672c\u52a8\u4f5c\u8fd8\u6ca1\u600e\u4e48\u5fd8\uff0c\u6c34\u91cc\u662f\u771f\u7684\u51b7\u554a\uff0c\u8001\u5e08\u53c8\u4e0d\u8ba9\u4e71\u6e38\u3002\u3002</p> <p>\u4eca\u5929C++\u8bfe\u5927\u6982\u628a\u6628\u5929\u9057\u7559\u4e0b\u6765\u7684svm\u95ee\u9898\u770b\u61c2\u4e86\uff0c\u4ee5\u540e\u8fd8\u8981\u591a\u590d\u4e60\u3002</p> <p>\u54c7\u54c8\u54c8\uff01\uff01\u7ec8\u4e8e\u628acuda\u88c5\u597d\u4e86\uff0c\u6211\u771f\u670d\u4e86\uff0c\u672c\u6765C\u76d8\u90fd\u6ee1\u4e86\uff0c\u88c5\u5b8ccuda\u4e4b\u540e\u53cd\u800c\u591a\u4e8610\u4e2aG\uff1f\u5f88\u795e\u5947\u5427\uff01</p> <p>\u590f\u8001\u5e08\u8ba9\u6211\u53c2\u52a0\u4ed6\u4eec\u7684\u8bfe\u9898\u7ec4\uff0c\u90fd\u5f00\u59cb\u4e3a\u56fd\u521b\u8d5b\u51c6\u5907\u4e86\u3002\u6ca1\u60f3\u5230\u53bb\u5e74\u8fd8\u5728\u5f53\u5fd7\u613f\u8005\uff0c\u4eca\u5e74\u5c31\u8981\u5e2e\u4e0a\u5fd9\u4e86\uff08\u867d\u7136\u4e0d\u4e00\u5b9a\u80fd\u5e2e\u4e0a\u5565\u5927\u5fd9\uff09</p> <p>\u8d34\u4e2a\u4f7f\u7528\u65f6\u95f4\uff1a</p> <p></p>"},{"location":"diaries/2025/20250305/","title":"3.5","text":"<p> \u7ea6 210 \u4e2a\u5b57  1 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>\u4eca\u5929\u4e0b\u5348\u628a\u7535\u8111\u6454\u4e86\u3002\u3002\u3002\u5fc3\u788e</p> <p>\u4e2d\u5348\u8e6d\u4e86\u81f4\u8fdc\u7684\u514d\u8d39\u5348\u9910\uff0c\u662f\u98df\u5176\u5bb6\uff0c\u611f\u89c9\u5473\u9053\u4e00\u822c\uff0c\u53ef\u80fd\u6211\u5403\u4e0d\u6765\u548c\u725b\u5427\u3002</p> <p>\u4eca\u5929\u4e0b\u5348\u5f00\u4e86\u5e74\u7ea7\u5927\u4f1a\uff0c\u5e74\u7ea7\u6700\u9ad8\u5206\uff08\u673a\u68b0\u7c7b\uff09\u7adf\u7136\u662f92.6\uff01\u592a\u725b\u4e86\uff0c\u6bd4\u6211\u9ad8\u4e86\u6574\u65741.4\u5206\uff0c\u679c\u7136\uff0c\u4eba\u5916\u6709\u4eba\uff0c\u5929\u5916\u6709\u5929\u3002\u6700\u540e\u4e13\u4e1a\u5206\u6d41\u5927\u6982\u7387\u56de\u53bb\u5de5\u4e1a\u5de5\u7a0b\u5427\uff0c\u56e0\u4e3a\u6211\u5b9e\u5728\u5bf9\u786c\u4ef6\u63d0\u4e0d\u8d77\u5174\u8da3\u3002\u8fd8\u662f\u504f\u7801\u65b9\u5411\u9002\u5408\u6211\u3002</p> <p>\u4eca\u5929\u665a\u4e0a\u7ec8\u4e8e\u89e3\u51b3\u4e86\u4e00\u5927\u56f0\u6270\u6211\u7684\u96be\u9898\uff0c\u591a\u4e8f\u4e86StackExchange\u3002jupyter notebook\u7ecf\u5e38\u5d29\u6e83\uff0c\u8bf4\u67d0\u4e2adll\u6587\u4ef6\u591a\u5f00\uff0c\u6700\u540e\u628anumpy\u5378\u8f7d\u53c8\u91cd\u88c5\u5c31\u89e3\u51b3\u4e86\u3002\u6211\u89c9\u5f97\u5f88\u795e\u5947\u554a\uff0c\u6211\u90fd\u7528\u7684pytorch\uff0c\u6ca1\u60f3\u5230numpy\u8fd8\u80fd\u6765\u5f71\u54cd\u3002</p> <p>\u770b\u4eca\u665a\u4e0a\u80fd\u4e0d\u80fd\u628a\u6570\u5b66\u539f\u7406\u5728\u8fdb\u4e00\u6b65\u7406\u89e3\u5427\u3002</p> <p></p>"},{"location":"notes/","title":"index","text":""},{"location":"notes/#notes","title":"Notes \ud83d\udcda","text":"<p>Abstract</p> <p>\u4e00\u4e9b\u6bd4\u8f83\u6210\u4f53\u7cfb\u7684\u7b14\u8bb0\u90fd\u505a\u5728\u8fd9\u91cc\uff0c\u65b9\u4fbf\u67e5\u9605\u3002</p> <p>\u672c\u90e8\u5206\u5185\u5bb9\uff08\u9664\u7279\u522b\u58f0\u660e\u5916\uff09\u91c7\u7528 \u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u4fdd\u6301\u4e00\u81f4 4.0 \u56fd\u9645 (CC BY-NC-SA 4.0) \u8bb8\u53ef\u534f\u8bae\u8fdb\u884c\u8bb8\u53ef\u3002</p>"},{"location":"notes/CS0501H/%E9%93%BE%E8%A1%A8/","title":"\u7ebf\u6027\u8868","text":"<p> \u7ea6 93 \u4e2a\u5b57  28 \u884c\u4ee3\u7801  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>\u5143\u7d20\u7684\u5730\u5740\uff1a\u5143\u7d20\u7684\u9996\u5730\u5740\uff08\u56e0\u4e3a\u4e00\u4e2a\u5143\u7d20\u53ef\u80fd\u662f\u591a\u4e2a\u5b57\u8282\uff09</p> <p>\u6bd4\u5982int\u7c7b\u578b\uff084\u4e2a\u5b57\u8282\uff09 a1 :0,1,2,3  a2:4,5,6,7</p> <p>\u968f\u673a\u8bfb\u53d6 \\(~O(1)\\) a[i-1] = a[0] + (i-1)* d</p> C++<pre><code>#define LIST_INIT_SIZE 100 //\u521d\u59cb\u5bb9\u91cf\n#define LIST_INCREMENT 10  //\u8ffd\u52a0\u5185\u5b58\u65f6\u7684\u589e\u91cf \u5b8f=\u5168\u5c40\u53d8\u91cf+const\ntypedef int ElemType;\n\ntypedef struct{\n    ElemType *elem; //\u8fde\u7eed\u5185\u5b58\u9996\u5730\u5740\n    int length;\n    int size;\n} SqList;\nSqList L:\nL.elem;//\u9996\u5730\u5740 \nL.elem[0];// \u7b2c\u4e00\u4e2a\u5143\u7d20\nl.length;//\u8868\u957f\nL.size; //\u8868\u5bb9\u91cf\nL.elem[L.length-1] // \u53d6\u6700\u540e\u4e00\u4e2a\u6570\n\nvoid init(SqList &amp;L){\n    L.elem = (ElemType *)malloc(LIST_INIT_SIZE * sizeof(ElemType))\n}\n\nvoid getElem(SqList L,int i,ElemType &amp;e)\n{\n    if (i &lt; 1 || i &gt; L.length)\n    {\n        return;\n    }\n    e = L.elem[i-1];\n}\n</code></pre> <p>\u94fe\u8868\uff1a\u89e3\u51b3\u63d2\u5165\u5220\u9664</p> <p>\u4e0d\u662f\u968f\u673a\u8bfb\u53d6</p> <p>\u5faa\u73af\u94fe\u8868\u5143\u7d20\u4e2a\u6570\uff1a(front-rear+\u5bb9\u91cf) mod \u5bb9\u91cf \u5176\u4e2dfront\u662f\u5934\u6307\u9488\uff0crear\u662f\u5c3e\u6307\u9488</p>"},{"location":"notes/EECS498/2-layer-network/","title":"\u7b2c\u4e8c\u6b21\u4f5c\u4e1a(Two Layer Net)","text":"<p> \u7ea6 633 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 3 \u5206\u949f</p> <p>\u4e3a\u4e86\u8bc1\u660e\u635f\u5931\u51fd\u6570 \\( L \\) \u5bf9\u6743\u91cd \\( W1 \\)\u3001\\( W2 \\) \u548c\u504f\u7f6e \\( b1 \\)\u3001\\( b2 \\) \u7684\u68af\u5ea6\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u94fe\u5f0f\u6cd5\u5219\uff08Chain Rule\uff09\u548c\u7ebf\u6027\u4ee3\u6570\u7684\u77e5\u8bc6\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u7684\u63a8\u5bfc\u8fc7\u7a0b\u3002</p>"},{"location":"notes/EECS498/2-layer-network/#l-w1-w2-b1-b2","title":"\u7528\u7ebf\u6027\u4ee3\u6570\u7684\u77e5\u8bc6\u8bc1\u660e \\( L \\) \u5bf9 \\( W1 \\)\u3001\\( W2 \\)\u3001\\( b1 \\)\u3001\\( b2 \\) \u7684\u68af\u5ea6","text":""},{"location":"notes/EECS498/2-layer-network/#1","title":"1. \u5b9a\u4e49\u7b26\u53f7\u548c\u7f51\u7edc\u7ed3\u6784","text":"<p>\u5047\u8bbe\u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\uff1a</p> <ol> <li>\u8f93\u5165\u5c42\uff1a\u8f93\u5165\u6570\u636e \\( X \\)\uff0c\u7ef4\u5ea6\u4e3a \\( N \\times D \\)\uff08\\( N \\) \u662f\u6837\u672c\u6570\u91cf\uff0c\\( D \\) \u662f\u7279\u5f81\u7ef4\u5ea6\uff09\u3002</li> <li>\u9690\u85cf\u5c42\uff1a<ul> <li>\u6743\u91cd \\( W1 \\)\uff0c\u7ef4\u5ea6\u4e3a \\( D \\times H \\)\uff08\\( H \\) \u662f\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u6570\u91cf\uff09\u3002</li> <li>\u504f\u7f6e \\( b1 \\)\uff0c\u7ef4\u5ea6\u4e3a \\( H \\)\u3002</li> <li>\u6fc0\u6d3b\u51fd\u6570\u4e3a ReLU\uff1a\\( h1 = \\text{ReLU}(X \\cdot W1 + b1) \\)\uff0c\u7ef4\u5ea6\u4e3a \\( N \\times H \\)\u3002</li> </ul> </li> <li>\u8f93\u51fa\u5c42\uff1a<ul> <li>\u6743\u91cd \\( W2 \\)\uff0c\u7ef4\u5ea6\u4e3a \\( H \\times C \\)\uff08\\( C \\) \u662f\u7c7b\u522b\u6570\u91cf\uff09\u3002</li> <li>\u504f\u7f6e \\( b2 \\)\uff0c\u7ef4\u5ea6\u4e3a \\( C \\)\u3002</li> <li>\u8f93\u51fa logits\uff1a\\( scores = h1 \\cdot W2 + b2 \\)\uff0c\u7ef4\u5ea6\u4e3a \\( N $\\times$ C \\)\u3002</li> </ul> </li> <li>Softmax \u548c\u635f\u5931\u51fd\u6570\uff1a<ul> <li>Softmax \u8f93\u51fa\uff1a\\( probs = \\text{Softmax}(scores) \\)\u3002</li> <li>\u4ea4\u53c9\u71b5\u635f\u5931\uff1a\\( L = -\\frac{1}{N} \\sum_{i=1}^N \\log(probs_{i, y_i}) )\uff0c\u5176\u4e2d \\( y_i \\) \u662f\u7b2c \\( i \\) \u4e2a\u6837\u672c\u7684\u771f\u5b9e\u6807\u7b7e\u3002</li> </ul> </li> </ol> <p>### 2. \u8ba1\u7b97\u68af\u5ea6</p> <p>\u6211\u4eec\u9700\u8981\u8ba1\u7b97 \\( L \\) \u5bf9 \\( W1 \\)\u3001\\( W2 \\)\u3001\\( b1 \\)\u3001\\( b2 \\) \u7684\u68af\u5ea6\u3002</p>"},{"location":"notes/EECS498/2-layer-network/#1-fracpartial-lpartial-w2-fracpartial-lpartial-b2","title":"(1) \u68af\u5ea6 \\( \\frac{\\partial L}{\\partial W2} ) \u548c ( \\frac{\\partial L}{\\partial b2} \\)","text":"<ul> <li>\u6839\u636e\u94fe\u5f0f\u6cd5\u5219\uff1a     [ \\(\\frac{\\partial L}{\\partial W2} = \\frac{\\partial L}{\\partial scores} \\cdot \\frac{\\partial scores}{\\partial W2}\\)     ]<ul> <li>\\( \\frac{\\partial L}{\\partial scores} = probs - \\text{one-hot label} \\)\uff0c\u7ef4\u5ea6\u4e3a \\( N \\times C \\)\u3002</li> <li>\\($ \\frac{\\partial scores}{\\partial W2} = h1^T$ \\)\uff0c\u7ef4\u5ea6\u4e3a \\( H $\\times$ N \\)\u3002</li> <li>\u56e0\u6b64\uff1a   [ \\(\\frac{\\partial L}{\\partial W2} = h1^T \\cdot (probs - \\text{one-hot label})\\)   ]</li> </ul> </li> <li>\u5bf9\u4e8e \\( b2 \\)\uff1a     [ \\(\\frac{\\partial L}{\\partial b2} = \\sum_{i=1}^N (probs_i - \\text{one-hot label}_i)\\)     ]</li> </ul> <p>#### (2) \u68af\u5ea6 \\(( \\frac{\\partial L}{\\partial W1} ) \u548c ( \\frac{\\partial L}{\\partial b1} )\\)</p> <ul> <li>\u6839\u636e\u94fe\u5f0f\u6cd5\u5219\uff1a     [ \\(\\frac{\\partial L}{\\partial W1} = \\frac{\\partial L}{\\partial h1} \\cdot \\frac{\\partial h1}{\\partial (X \\cdot W1 + b1)} \\cdot \\frac{\\partial (X \\cdot W1 + b1)}{\\partial W1}\\)     ]<ul> <li>\\( \\frac{\\partial L}{\\partial h1} = (probs - \\text{one-hot label}) \\cdot W2^T \\)\uff0c\u7ef4\u5ea6\u4e3a \\( N \\times H \\)\u3002</li> <li>\\( \\frac{\\partial h1}{\\partial (X \\cdot W1 + b1)} = \\text{ReLU \u7684\u5bfc\u6570} \\)\uff0c\u5373 \\( 1 \\) \u5982\u679c \\( h1 &gt; 0 \\)\uff0c\u5426\u5219 \\( 0 \\)\u3002</li> <li>\\( \\frac{\\partial (X \\cdot W1 + b1)}{\\partial W1} = X^T )\uff0c\u7ef4\u5ea6\u4e3a \\( D \\times N \\)\u3002</li> <li>\u56e0\u6b64\uff1a   [ \\(\\frac{\\partial L}{\\partial W1} = X^T \\cdot \\left( (probs - \\text{one-hot label}) \\cdot W2^T \\odot \\text{ReLU \u7684\u5bfc\u6570} \\right)\\)   ]</li> </ul> </li> <li>\u5bf9\u4e8e \\( b1 \\)\uff1a     [ \\(\\frac{\\partial L}{\\partial b1} = \\sum_{i=1}^N \\left( (probs_i - \\text{one-hot label}_i) \\cdot W2^T \\odot \\text{ReLU \u7684\u5bfc\u6570} \\right)\\)     ]</li> </ul> <p>### 3. \u6b63\u5219\u5316\u9879\u7684\u68af\u5ea6</p> <p>\u5982\u679c\u635f\u5931\u51fd\u6570\u5305\u542b L2 \u6b63\u5219\u5316\u9879\uff08\u5982 \\( \\text{reg} \\cdot (\\|W1\\|^2 + \\|W2\\|^2) \\)\uff09\uff0c\u5219\u68af\u5ea6\u9700\u8981\u52a0\u4e0a\u6b63\u5219\u5316\u9879\u7684\u5bfc\u6570\uff1a</p> <ul> <li>\u5bf9\u4e8e \\( W1 \\)\uff1a     [ \\(\\frac{\\partial L}{\\partial W1} += 2 \\cdot \\text{reg} \\cdot W1\\)     ]</li> <li>\u5bf9\u4e8e \\( W2 \\)\uff1a     [ \\(\\frac{\\partial L}{\\partial W2} += 2 \\cdot \\text{reg} \\cdot W2\\)     ]</li> </ul>"},{"location":"notes/EECS498/2-layer-network/#4","title":"4. \u603b\u7ed3","text":"<p>\u901a\u8fc7\u94fe\u5f0f\u6cd5\u5219\u548c\u7ebf\u6027\u4ee3\u6570\u7684\u77e5\u8bc6\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u4e86\u635f\u5931\u51fd\u6570 \\( L \\) \u5bf9 \\( W1 \\)\u3001\\( W2 \\)\u3001\\( b1 \\)\u3001\\( b2 \\) \u7684\u68af\u5ea6\u516c\u5f0f\u3002\u8fd9\u4e9b\u516c\u5f0f\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u3002</p>"},{"location":"notes/EECS498/A1/","title":"\u7b2c\u4e00\u6b21\u4f5c\u4e1a(pytorch & KNN)","text":"<p> \u7ea6 167 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>Deep Learning \\(\\in\\) Machine Learning</p> <p>Deep  Learning \\(\\cap\\) Computer Vision = EECS 498</p> <p>SIFT:\u8bc6\u522b\u56fe\u7247\u4e2d\u5c0f\u7684\u5339\u914d\u70b9\uff0c\u8ba1\u7b97\u7279\u5f81\u5411\u91cf\uff08\u4e0d\u4f1a\u56e0\u4e3a\u65cb\u8f6c\u4ee5\u53ca\u56fe\u7247\u7684\u4eae\u5ea6\u6539\u53d8\uff09</p> <p>\u79bb\u73b0\u5b9e\u751f\u6d3b\u8fd8\u6709\u4e00\u5b9a\u8ddd\u79bb\uff1a\u8ba1\u7b97\u673a\u53ea\u80fd\u770b\u5230\uff08\u8bc6\u522b\uff09\u4fe1\u606f\uff0c\u5374\u4e0d\u80fd\u7406\u89e3\u56fe\u7247\u80cc\u540e\u7684\u542b\u4e49\u3002\u4eba\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u4fe1\u606f\u6765\u63a8\u7406\u5224\u65ad\uff0c\u4f46\u8ba1\u7b97\u673a\u5f88\u96be\u505a\u5230\u3002</p> <p>x.min(dim=0)\uff1a\u4f1a\u8fd4\u56de\u4e24\u4e2a\u503c\uff0c\u7b2c\u4e00\u4e2a\u5c31\u662f\u6700\u5c0f\u503c\uff0c\u7b2c\u4e8c\u4e2a\u662f\u6700\u5c0f\u503c\u7684\u4f4d\u7f6e\uff08\u7d22\u5f15\uff09</p> <p>\u5982\u679c\u53ea\u60f3\u83b7\u5f97\u7d22\u5f15\u7684\u8bdd\uff0c\u5c31\u7528x.argmin()</p> <p>\u4e0b\u8f7dpytorch\u6559\u7a0b\u6587\u4ef6</p> <p>\u4e0b\u8f7dKNN\u7b97\u6cd5\u6587\u4ef6</p> <p>\u67e5\u770b Pytorch \u4ea4\u4e92 Notebook</p> <p>\u67e5\u770b KNN \u4ea4\u4e92 Notebook</p>"},{"location":"notes/EECS498/A2/","title":"\u7b2c\u4e8c\u6b21\u4f5c\u4e1a(Linear Classifiers)","text":""},{"location":"notes/EECS498/A2/#svm","title":"SVM:","text":"<p> \u7ea6 1319 \u4e2a\u5b57  22 \u884c\u4ee3\u7801  1 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 7 \u5206\u949f</p> <p>\u200b   \\(W:(D,C)\\) \\(X:(N,D)\\) \\(y:(N,)\\)</p> <p>\u6ce8\uff1a1. (N,)\u8868\u793a\u4e00\u7ef4\u5411\u91cf\uff0c\u65e2\u4e0d\u662f\u884c\u5411\u91cf\u4e5f\u4e0d\u662f\u5217\u5411\u91cf\u3002\u5982\u679c\u8981\u663e\u5f0f\u7684\u8868\u793a\u884c\u5411\u91cf(1,N)\u6216\u8005\u5217\u5411\u91cf(N,1),\u5c31\u8981\u7528.view(1,-1)\u6216\u8005.view(-1,1)\u3002</p> <ol> <li>\\(f(x_i,W)=W^Tx_i\\) \u5176\u4e2d\\(x_i\\)\u8868\u793a\\(X[i]\\),\u5373\\(X\\)\u7684\u7b2c\\(i\\)\u884c\uff0c\u662f\u4e2a\u4e00\u7ef4\u5411\u91cf\\((D,)\\),\u4e0d\u662f\u884c\u5411\u91cf\uff01\uff08\u5373\u4f60\u53d6\u884c/\u5217\u90fd\u4f1a\u964d\u7ef4\uff09</li> <li>\u4e0d\u7ba1\u662f\u4ec0\u4e48\u4e58\u6cd5\uff08\u77e9\u9635\u548c\u77e9\u9635\uff0c\u77e9\u9635\u548c\u5411\u91cf\uff0c\u70b9\u4e58\u8fd8\u662f\u77e9\u9635\u4e58\u6cd5\uff09\uff0c\u90fd\u7528matmul()\u5c31\u597d\u4e86\uff0c\u514d\u5f97\u62a5\u9519</li> <li>\\(W\\) \u4e2d\u884c\u6570\\(D\\)\u4ee3\u8868\u7279\u5f81\u6570\uff0c\u4e00\u822c\u4e3aRGB\u901a\u9053+\u50cf\u7d20\u503c\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u8fd9\u4e2a\u7279\u5f81\u6570\u6781\u5927\uff0c\u5305\u542b\u4e86\u50cf\u7d20\u7684\u6240\u6709\u4fe1\u606f\uff1b\u5217\u6570\\(C\\)\u8868\u793a\u79cd\u7c7b\u6570\u76ee\uff0c\u5982CIFR-10\u63d0\u4f9b\u4e8610\u79cd\u56fe\u7247\u7c7b\u522b\u3002\\(X\\)\u7684\u884c\u6570\\(N\\)\u8868\u793a\u6837\u672c\u7684\u6570\u91cf\uff0c\u56e0\u6570\u636e\u5e93\u7684\u5927\u5c0f\u4e0d\u540c\u800c\u4e0d\u540c\u3002</li> </ol> <p>\u8fd9\u91cc\u76f4\u63a5\u501f\u7528@z_z\u5927\u4f6c\u7684\u56fe\uff1a</p> <p></p> <p>\\(Loss\\)\u8bb0\u5f97\u8981\u5e73\u5747\uff0c\u800c\u4e14\u6bcf\u6b21\u90fd\u662f\u7d2f\u52a0\uff0c\u4e0d\u662f\u8d4b\u503c\uff0c\u5426\u5219\u90fd\u4f1a\u88ab\u5faa\u73af\u6700\u540e\u4e00\u6b21\u7684\u53d6\u503c\u7ed9\u8986\u76d6\u3002\u8fd8\u8981\u8bb0\u5f97\u6b63\u5219\u5316\uff0c\u4e00\u822c\u662f\\(L_2\\)norm\uff08\u4e0d\u7528\u5f00\u6839\u53f7\uff01\u53ea\u9700\u8981\u53d6\u6bcf\u4e2a\u5143\u7d20\u7684\u5e73\u65b9\u518d\u4e58\u4ee5\u6b63\u5219\u5316\u7cfb\u6570\uff09\uff0c\u5373\\(R(W)=R\\times \\abs{W^2}\\)</p> <p>\\(dW\\)\u7684\u8ba1\u7b97\u65b9\u6cd5: \u6ce8\u610f\uff0c\u6c42\u5bfc\u662f\u5bf9\\(W\\)\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u800c\u8a00\u7684\u3002\u5373\\(W\\)\u4e2d\u6bcf\u4e2a\u5143\u7d20\u53d8\u5316\u4e00\u70b9\uff0c\u6574\u4e2a\\(L\\)\u635f\u5931\u51fd\u6570\u7684\u53d6\u503c\u4f1a\u5982\u4f55\u53d8\u5316\u3002\u6211\u4eec\u5173\u6ce8\u7684\u662f\\(W\\)\u800c\u4e0d\u662f\\(X\\)\uff01\u56e0\u4e3a\\(W\\)\u624d\u662f\u6211\u4eec\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u3002</p> <p>\u5bf9\u4e8eSVM\u800c\u8a00\uff0c\\(L_i = \\sum_{j\\neq y_i}\\max(0,s_j-s_{y_i}+1)\\)</p>"},{"location":"notes/EECS498/A2/#_1","title":"\u5faa\u73af\u6cd5","text":"<p>\u8ba1\u7b97\\(\\frac{\\partial L_i}{dW}\\)\u9700\u8981\u5206\u4e24\u79cd\u60c5\u51b5\uff0c\u4f46\u662f\u603b\u4f53\u601d\u60f3\u90fd\u662f\u7528\u94fe\u5f0f\u6cd5\u5219</p> <p>\\(\\frac{\\partial L_i}{\\partial W} = \\frac{\\partial L_i}{\\partial s_i}\\frac{\\partial s_i}{dW}\\) \\(s_i\\)\u4ee3\u8868\u5bf9\u4e8e\u7b2c\\(i\\)\u4e2a\u6837\u672c\u6240\u5f97\u5230\u7684score\uff0c\u4f7f\u7528\\(W^Tx_i\\)\u8ba1\u7b97\u5f97\u5230\u7684\uff0c\u8fd9\u6837\u7684\u597d\u5904\u662f\u6211\u4eec\u5f88\u5bb9\u6613\u77e5\u9053\\(\\frac{\\partial s_i}{\\partial W}\\)\u7b49\u4e8e\\(x_i\\)</p> <p>\u63a8\u5bfc\uff1a\u5bf9\u4e8e\u67d0\u4e2a\u786e\u5b9a\u7684\\(i\\)\u800c\u8a00\uff0c\\(s_i=W^T x_i\\),\\(ds_i=d(W^T)x_i+W^Tdx_i=(dW)^Tx_i\\),\u4e24\u8fb9\u540c\u65f6\u53d6\u8ff9,\u6709\\(s_i=tr(s_i)=tr((dW)^Tx_i)=tr(x_i^TdW)\\),\u56e0\u6b64\\(\\frac{\\partial s_i}{\\partial W}=x_i\\)\u3002\u5f53\\(j=y[i]\\)\u65f6\uff0c\u8bf4\u660e\u662f\u6b63\u786e\u7684\u6807\u7b7e\uff0c\u5bf9\u635f\u5931\u51fd\u6570\u8d21\u732e\u4e3a0\uff0c\u81ea\u7136\u5bfc\u6570\u4e5f\u4e3a0.\u5f53\\(j\\neq y_i\\)\u4e14\\(L_i&gt;0\\)\u65f6\uff0c\u5bf9\u4e8e\u6b63\u786e\u7c7b\u522b\u800c\u8a00\uff0c\u7531\\(L_i = \\sum_{j\\neq y_i}\\max(0,s_j-s_{y_i}+1)\\)\u4e0d\u96be\u770b\u51fa\uff0c\\(\\frac{\\partial L}{\\partial s_{y_i}}=-1\\),\u56e0\u6b64\u6709\u94fe\u5f0f\u6cd5\u5219\u76f8\u4e58\uff0c\u68af\u5ea6\u51cf\u53bb\\(-X[i]\\)\u3002\u540c\u7406\uff0c\u5bf9\u4e8e\u9519\u8bef\u7c7b\u522b\uff0c\u5bfc\u6570\u4e3a1\uff0c\u90a3\u4e48\u68af\u5ea6\u5e94\u8be5\u52a0\u4e0a\\(X[i]\\)\u3002</p> <p>\u4ee3\u7801\u5b9e\u73b0:</p> Python<pre><code>def svm_loss_naive(W: torch.Tensor, X: torch.Tensor, y: torch.Tensor, reg: float):\n    dW = torch.zeros_like(W)  # initialize the gradient as zero\n    num_classes = W.shape[1]\n    num_train = X.shape[0]\n    loss = 0.0\n    for i in range(num_train):\n        scores = W.t().mv(X[i])\n        correct_class_score = scores[y[i]]\n        for j in range(num_classes):\n            if j == y[i]:\n                continue\n            margin = scores[j] - correct_class_score + 1  \n            if margin &gt; 0:\n                loss += margin\n                dW[:,y[i]] -= X[i]\n                dW[:,j] += X[i]\n    loss /= num_train\n    loss += reg * torch.sum(W * W)\n    dW_regular = 2 * reg * W\n    dW += dW_regular\n    return loss, dW\n</code></pre>"},{"location":"notes/EECS498/A2/#_2","title":"\u5411\u91cf\u5316\u6cd5","text":"<p>\u7279\u70b9\uff1a\u76f4\u63a5\u5bf9\u6574\u4e2a\u77e9\u9635\u8fdb\u884c\u64cd\u4f5c\uff0c\u800c\u4e0d\u662f\u62bd\u67d0\u4e00\u884c/\u5217\u3002\uff08\u7701\u65f6\uff0c\u5e76\u53d1\uff09</p> <p>\u4e3a\u4e86\u4e0d\u4f7f\u7528\u5faa\u73af\u800c\u9009\u51fa\u6bcf\u4e00\u4e2a\u6837\u672c\u6b63\u786e\u7c7b\u522b\u5bf9\u5e94\u7684\u5206\u6570\uff0c\u91c7\u7528\u4e00\u4e2a\u975e\u5e38\u5de7\u5999\u7684\u529e\u6cd5\u53d6\u51fa\u6570\u636e\uff0c\u90a3\u5c31\u662f\u8ba9\u884c\u6570\u548c\u5217\u6570\u4e00\u4e00\u5bf9\u5e94\uff0c\u5f62\u6210\u6709\u5e8f\u7684\u6570\u5bf9\uff08\u7c7b\u4f3czip\uff09\uff0c\u5373\u884c\u548c\u5217\u4e24\u4e2a\u4f4d\u7f6e\u4e0d\u653e\u6570\u5b57\uff0c\u653e\u4e24\u4e2a\u5f62\u72b6\u76f8\u540c\u7684tensor\u3002\u8fd9\u91cc\u6211\u4eec\u5e0c\u671b\u5728\u7b2c\u4e00\u4e2a\u6837\u672c\u53d6\u51fa\u6570\u636e\u540e\u81ea\u52a8\u5230\u7b2c\u4e8c\u4e2a\u6837\u672c\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u5efa\u7acb\u81ea\u7136\u6570(0~N-1)\u4e0e\u6b63\u786e\u6807\u7b7ey\u7684\u4e00\u4e00\u6620\u5c04\uff0c\u524d\u8005\u53ef\u4ee5\u7528torch.arange(N)\u5b9e\u73b0\u3002\u4f46\u662f\u53d6\u51fa\u6765\u4e4b\u540e\u662f\u4e2a\u4e00\u7ef4\u5f20\u91cf\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5176reshape\u6210\u4e8c\u7ef4\u5217\u5411\u91cf(N,1)\u3002</p> <p>\u8ba1\u7b97\u635f\u5931\u77e9\u9635(\u4e0d\u59a8\u8bbe\u4e3amargin)\u65f6\u53ef\u4ee5\u5145\u5206\u5229\u7528\u5e7f\u64ad\u673a\u5236\u4ee3\u66ff\u5faa\u73af\uff0c\u5e7f\u64ad\u673a\u5236\u4f1a\u5c06(N,1)\u590d\u5236\u6210(N,C),\u6bcf\u4e00\u884c\u90fd\u662f\u76f8\u540c\u7684\u6570\u5b57\uff0c\u4e5f\u5c31\u662f\u6b63\u786e\u5e8f\u53f7\u5bf9\u5e94\u7684\u5206\u6570\u3002\u6211\u4eec\u9700\u8981\u5c06\u6700\u5c0f\u503c\u8bbe\u62100\uff0c\u5c0f\u4e8e0\u7684\u8bf4\u660e\u4e0d\u4f1a\u5bf9\u635f\u5931\u51fd\u6570\u9020\u6210\u5f71\u54cd\uff0c\u800c\u5927\u4e8e0\u7684\u6211\u4eec\u9700\u8981\u6ce8\u610f\uff1a\u6709\u4e24\u79cd\u60c5\u51b5\uff0c\u4e00\u79cd\u662f\u672c\u6765\u5c31\u662f\u5bf9\u7684\uff0c\u90a3\u4e48\u7ecf\u8fc7\\(L_i = \\sum_{j\\neq y_i}\\max(0,s_j-s_{y_i}+1)\\)\u540e\u4f1a\u5f97\u52301\uff0c\u8fd9\u79cd\u5f71\u54cd\u6211\u4eec\u9700\u8981\u624b\u52a8\u6392\u9664\uff0c\u56e0\u6b64\u9700\u8981\u76f4\u63a5\u8d4b\u503c\u4e3a0\uff08\u8fc7\u7a0b\u540c\u4e0a\u8ff0\u53d6\u6570\u636e\uff09\uff1b\u5982\u679c\u662f\u9519\u7684\u90a3\u5c31\u4f1a\u5bf9\u635f\u5931\u51fd\u6570\u9020\u6210\u5f71\u54cd\u3002\u6700\u540e\u522b\u5fd8\u4e86\u635f\u5931\u51fd\u6570\u662f\u4e2a\u6807\u91cf\uff0c\u9700\u8981\u5c06\u77e9\u9635\u5404\u5143\u7d20\u52a0\u548c\u6c42\u5e73\u5747\u518d\u6b63\u5219\u5316\u3002</p> <p>\u800c\u8ba1\u7b97\\(dW\\)\u662f\u4e00\u4e2a\u96be\u70b9\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u529e\u6cd5\u518d\u7528\u5faa\u73af\u7684\u64cd\u4f5c\u4e86\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u80fd\u60f3\u529e\u6cd5\u7528\u77e9\u9635\u4e58\u6cd5\u6765\u4ee3\u66ff\u5faa\u73af\uff08\u6bd4\u5982\u8bf4\u5982\u679c\u6211\u4eec\u60f3\u8981\u4ea4\u6362\u77e9\u9635\u7684\u76f8\u90bb\u4e24\u5217\uff0c\u6211\u4eec\u4e0d\u7528\u5faa\u73af\u7684\u8bdd\u53ef\u4ee5\u7528\u5355\u4f4d\u77e9\u9635\u8fdb\u884c\u76f8\u540c\u7684\u53d8\u6362\u540e\u53f3\u4e58\u5f97\u5230\uff09</p> <p>\u6211\u4eec\u5148\u660e\u786e\uff1a\u5bf9\u4e8emargin\u800c\u8a00\uff0c\u5927\u4e8e0\u7684\u8bf4\u660e\u662f\u9519\u7684\uff0c\u68af\u5ea6\u5e94\u8be5\u4e3a1\uff1b\u800c\u5bf9\u4e8e\u672c\u8eab\u6b63\u786e\u7684\u5e8f\u53f7\uff0c\u7531\u4e8e\u4e00\u884c\u91cc\u6709\u591a\u5c11\u6b21margin&gt;0\u5c31\u6709\u591a\u5c11\u6b21\\(-s_{y_i}\\),\u6240\u4ee5\u68af\u5ea6\u9700\u8981\u51cf\u53bb\u5927\u4e8e0\u7684\u5143\u7d20\u4e2a\u6570\uff08\u7528sum,\u4e14\u6307\u5b9adim=1\uff0cdim=1\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u6d88\u5931\uff0c\u53ea\u5269\u4e0b\u7b2c\u4e00\u7ef4\u5ea6\uff08\u5982\u679c\u7528\u4e86keepdim=True\u4ee3\u8868\u4fdd\u7559\u6c42\u548c\u7684\u90a3\u4e2a\u7ef4\u5ea6\uff0c\u53ea\u662f\u7ef4\u5ea6\u53d8\u62101\uff09\uff09</p> <p>\u540e\u9762\u6211\u81ea\u5df1\u6ca1\u641e\u61c2\uff0c\u5148\u8d34\u4e00\u4e2a\u652f\u6301\u5411\u91cf\u673a.pdf by z_z</p> <p>\u6211\u81ea\u5df1\u53c8\u60f3\u660e\u767d\u4e86\uff0c\u53ef\u80fd\u540e\u9762\u5f97\u591a\u56de\u987e\u3002SVM&amp;SOFTMAX by 6ch.</p>"},{"location":"notes/EECS498/Back%20Propagation/","title":"\u7b2c\u56db\u8282(Back Propagation)","text":"<p> \u7ea6 487 \u4e2a\u5b57  18 \u884c\u4ee3\u7801  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 3 \u5206\u949f</p> <p>Back Propagation:</p> <p>Simple Example:\\(f(x,y,z)=(x+y)z\\)</p> <p>1.Forward Pass: Compute outputs from left to right(from inputs to outputs)</p> <p>\\(q=x+y\\) \\(f=qz\\)</p> <p>2.Backward Pass: Compute derivatives </p> <p>Want: \\(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}\\)</p> <p>Order: \\(\\frac{\\partial f}{\\partial f}\\rightarrow \\frac{\\partial f}{\\partial z}\\rightarrow \\frac{\\partial f}{\\partial q}\\rightarrow \\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}\\rightarrow \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\\)</p> <p>[Downstream] = [Local]*[Upstream]</p> <p>sigmoid function:\\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\)</p> <p>\\(\\frac{d\\sigma(x)}{dx}=(1-\\sigma(x))\\sigma(x)\\)</p> <p>add: don\u2019t change derivatives so the elements linked by the \u201c+\u201d share the same gradient.</p> <p>copy: one side\u2019s derivatives add to the other side.</p> <p>multiply: swap multiplier</p> <p>max: gradient router,that reduces one element to \\(0\\) and another,full gradient the same with other side\u2019s element\u2019s.</p> <p>Flat: \u201cReverse Thinking Method\u201d:</p> Python<pre><code>#Forward\ndef f(w0,x0,w1,x1,w2):\n    s0 = w0 * x0\n    s1 = w1 * x1\n    s2 = s0 + s1\n    s3 = s2+ w2\n    L = sigmoid(s3)\n#Backward\n    grad_L = 1.0\n    grad_s3 = grad_L * (1-L) * L    #sigmoid function has a special form of derivatives\n    grad_w2 = grad_s3   #gradient copier\n    grad_s2 = grad_s3   #gradient copier\n    grad_s0 = grad_s2   #gradient copier\n    grad_s1 = grad_s2   #gradient copier\n    grad_w1 = grad_s1 * x1  #gradient multiplier\n    grad_x1 = grad_s1 * w1  #gradient multiplier\n    grad_w0 = grad_s0 * x0  #gradient multiplier\n    grad_x0 = grad_w0 * w0  #gradient multiplier\n</code></pre> <p>\\(y \\in \\mathbb{R},x \\in \\mathbb{R}^M,\\frac{dy}{dx}\\in \\mathbb{R}^M\\)</p> <p>\\(y \\in \\mathbb{R}^N,x \\in \\mathbb{R}^M,\\frac{dy}{dx}\\in \\mathbb{R}^{M\\times N}\\) :Jacobian Matrix</p> <p>4D INPUT \\(x\\):[1,-2,3,-3]  $\\rightarrow f(x)=\\max(0,x)(elementwise)\\rightarrow $  4D OUTPUT \\(y\\) = [1,0,3,0]</p> <p>4D \\(\\frac{dL}{dy}\\):[4,-1,5,9] $\\rightarrow $ \\(\\frac{dy}{dx}\\frac{dL}{dy}\\)</p> <p>\\(\\frac{dy}{dx}\\)=\\(\\begin{bmatrix}1&amp;0&amp;0&amp;0 \\\\0&amp;0&amp;0&amp;0 \\\\0&amp;0&amp;1&amp;0\\\\0&amp;0&amp;0&amp;0  \\end{bmatrix}\\)positive: 1 negative: 0</p> <p>Jacobian is sparse!: off-diagonal entries all zero! When doing a big Jacobian Matrix Multiply,it\u2019ll cause large resources waste because almost every element is zero!Never explicitly form Jacobian,instead use implicit multiplication.</p> <p>\\(y=xw\\)(\\(x:[N\\times D]\\) \\(w:[D\\times M]\\) \\(y:[N \\times M]\\))</p> <p>\\(\\frac{dL}{dx_{i,j}}=\\frac{dy}{dx_{i,j}}\\cdot\\frac{dL}{dy}=w_{j,:}\\cdot\\frac{dL}{dy_{i,:}}\\)</p> <p>\\(\\frac{dL}{dx}=\\frac{dL}{dy}w^T\\)(How to remember? Use the shape!)</p> <p>\\(\\frac{dL}{dw}=x^T\\frac{dL}{dy}\\)</p> <p>Hint:     \\(\\cdot\\)  means inner product while blank space means matrix multiply</p> <p>Backward-Mode: A vector input and a scalar output</p> <p>Forward-Mode: A scalar input and a vector output</p> <p>Compute Higher-Order Derivatives(Cool!):</p> <p>\\(x_0 --f1--&gt;x1--f2--&gt;L--f_2'--&gt;\\frac{dL}{dx_1}--f_1'--&gt;\\frac{dL}{dx_0}--\\cdot v--&gt;\\frac{dL}{dx_0}\\cdot v\\)</p> <p>we want to calculate \\(\\frac{\\partial^2L}{\\partial x_0^2}\\)  then we can calculate\\(\\frac{\\partial^2L}{\\partial x_0^2}\\cdot v\\) ,  and surprisingly,\\(\\frac{\\partial^2L}{\\partial x_0^2}\\cdot v=\\frac{\\partial}{\\partial x_0}[\\frac{\\partial L}{\\partial x_0}\\cdot v]\\)</p> <p>(\\(v\\) is independent from \\(x_0\\)) </p> <p>use backprop we will get the answer(remember: backprop gets the derivatives of output with regard to the input)</p> <p>\u518d\u5206\u6790\uff1a\u5012\u7740\u5199\u4e00\u904d</p> <p>\u53c2\u8003\u94fe\u63a5\uff1ahttps://zhuanlan.zhihu.com/p/21407711</p>"},{"location":"notes/EECS498/Derivative%20of%20Matrix/","title":"\u77e9\u9635\u6c42\u5bfc","text":"<p> \u7ea6 228 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 1 \u5206\u949f</p> <p>\u4e00\u5143\u5fae\u79ef\u5206\u4e2d\u7684\u5bfc\u6570\u4e0e\u5fae\u5206\u7684\u5173\u7cfb: \\(df = f'(x)dx\\)</p> <p>\u591a\u5143\u5fae\u79ef\u5206\u4e2d\u7684\u68af\u5ea6(\u5217\u5411\u91cf)\u4e0e\u5fae\u5206\u7684\u5173\u7cfb: \\(df = \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}dx_i=\\frac{\\partial f}{\\partial \\textbf{x}}^Td\\textbf{x}\\) \u7b2c\u4e00\u4e2a\u7b49\u53f7\u662f\u5168\u5fae\u5206\u516c\u5f0f\uff0c\u7b2c\u4e8c\u4e2a\u7b49\u53f7\u8868\u8fbe\u4e86\u68af\u5ea6\u5411\u91cf(\\(n \\times 1\\))\u4e0e\u5fae\u5206\u5411\u91cf(\\(n \\times 1\\))\u7684\u5185\u79ef\u662f\u5168\u5fae\u5206\u3002\u7531\u6b64\uff0c\u6211\u4eec\u77e5\u9053\\(df=\\sum_{i=1}^m\\sum_{j=1}^n\\frac{\\partial f}{\\partial X_{ij}}dX_{ij} = \\trace(\\frac{\\partial f}{\\partial X}^TdX)\\) </p> <p>\u7b2c\u4e00\u4e2a\u7b49\u53f7\u662f\u5168\u5fae\u5206\u516c\u5f0f\uff0c\u7b2c\u4e8c\u4e2a\u7b49\u53f7\u5efa\u8bae\u5728\u7eb8\u4e0a\u63a8\u5bfc\u4e00\u904d\uff0c\u56e0\u4e3a\\(\\trace(A^TB)=\\sum_{i,j}A_{ij}B_{ij}=\\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij}\\) \u6240\u4ee5\u8bf4\u7167\u846b\u82a6\u753b\u74e2\u80fd\u591f\u5f97\u5230\u3002\uff08\u8fd9\u91cc\u53ef\u4ee5\u4fa7\u9762\u8bf4\u660e\u7ebf\u6027\u4ee3\u6570\u4e2d\u5b9a\u4e49\\(tr(A^TB)\\)\u4e3a\u77e9\u9635\u5185\u79ef\u7684\u5408\u7406\u6027\uff09</p> <p>\u521b\u5efa\u77e9\u9635\u5fae\u5206\u7684\u8fd0\u7b97\u6cd5\u5219\u77e9\u9635\u6c42\u5bfc.pdf by 6ch.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/","title":"\u7b2c\u4e00\u8282(Linear Classifiers)","text":"<p>Linear Classification: \\(f(x_i,W) = W \\cdot x\\)</p> <p>Matrix multiply: stretch x to a one-dimension vector,W is a matrix.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/#multiclass-svm-loss","title":"Multiclass SVM Loss:","text":"<p> \u7ea6 366 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> <p>Let \\(f(x_i,W)\\) be scores,then the SVM scores has the form: \\(L_i = \\sum_{j\\neq y_i}\\max(0,s_j-s_{y_i}+1)\\)</p> <p>\\(s_{y_i}\\) is the correct label\u2019s score,while \\(s_j\\) is the wrong label\u2019s scores. When \\(s_j\\) is larger than \\(s_{y_i} - 1\\)</p> <p>,that means it contributes to the loss,so that \\(L_i\\) is greater than \\(0\\).</p> <p>Characteristics: 1.When give the \\(s_{y_i}\\) a little bit change,the Loss function will not change. Because after change,\\(s_{y_i}\\) is still 1 more than the wrong label\u2019s scores.</p> <p>min possible : 0 max:\\(+\\infty\\)</p> <p>When all scores are small random values,loss is \\(C - 1\\)(\\(s_j \\approx s_{y_i}\\)) where C stands for the number of categories.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/#regularization","title":"Regularization","text":"<p>\\(L(W)=\\frac{1}{N}\\sum_{i=1}^NL_i(f(x_i,W),y_i)+\\lambda R(W)\\) </p> <p>The most common regularization: L2-norm \\(\\sum_i\\sum_jW_{i,j}^2\\) </p> <p>Why we need that?:</p> <ul> <li> <p>Express preferences in among models beyond \u201cminimize training error\u201d,allow people to integrate their wisdom and knowledge they\u2019ve already obtained.</p> </li> <li> <p>Avoid overfitting </p> </li> </ul> <p>Example: \\(x = [1,1,1,1] \\newline w_1=[1,0,0,0] \\newline w_2=[0.25,0.25,0.25,0.25]\\)</p> <p>It\u2019s obvious that \\(w_1^\\mathrm T \\cdot x = w_2^\\mathrm T\\cdot x = 1\\)</p> <p>L2-norm regularization prefer more balanced matrix,which is \\(w_2\\) in this example. This implies that use as many functions as possible in this preference.\u201dspread out the weights\u201d</p> <p>prefer simple models: Occam's Razor reveals the truth that simplicity is much preferred.</p>"},{"location":"notes/EECS498/Linear%20Classifiers/#cross-entropy-loss","title":"Cross Entropy Loss","text":"<p>SoftMax function: </p> cat 3.2 24.5 0.13 car 5.1 164.0 0.87 frog -1.7 0.18 0.00 <p>\u200b               unnormalized log-prob/logits --exp\u2192 unnormalized prob --normalize\u2192probabilities</p> <p>\\(L_i = -\\ln P(Y = y_i |X = x_i)\\)  Maximum Likelihood Estimation</p> <p>min possible loss:0 (it can only approach to 0 but never truly reach)   max:\\(+\\infty\\)</p> <p>When all scores are small random values,loss is \\(-\\ln C\\) where C stands for the number of categories.</p>"},{"location":"notes/EECS498/Neural%20Network/","title":"\u7b2c\u4e09\u8282(Neural Network)","text":"<p> \u7ea6 319 \u4e2a\u5b57  16 \u884c\u4ee3\u7801  5 \u5f20\u56fe\u7247  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f</p> <p>Linear Classifiers cannot deal with non-linear boundaries.</p> <p>such as two circles with different radius but the same center point.</p> <p>\u2192 use polar coordinate to create a feature space. \\(r\\) is the x axis and the \\(\\theta\\) is the y axis.</p> <p>all points on the same circle is seated on the same vertical line that is parallel to the y axis.</p> <p>(Before) Linear score function: only a small part of Feature Extraction can adjust itself to better maximizing its ability.</p> <p>Learn only one template of one category.</p> <p>(After) Neural Network: raw picture pixel \u2192 classification scores</p> <p>Learn several templates of one category.</p> <p>Linear score function: \\(f = Wx\\)</p> <p>2-layer Neural Network:\\(f=W_2\\max(0,W_1x)\\)</p> <p>\u200b               \\(W_2 \\in \\mathbb{R}^{C\\times H} \\:  W_1 \\in \\mathbb{R}^{H\\times D}\\:  x \\in \\mathbb{R}^D\\)</p> <p>\\(h = W_1x = (\\alpha_1 ,\\alpha_2,\\cdots,\\alpha_H)^Tx\\)</p> <p>Element \\((i,j)\\) of \\(W_1\\) gives the effect on \\(h_i\\) from \\(x_j\\)</p> <p>Deep Neural Networks: Depth = number of layers = number of Matrix</p> <p>\u200b   Width = Size of each layer</p> <p>Activation Functions:</p> <p>Without the activation function,we will go back to \\(f=W_2W_1x=Wx\\) which is linear classifiers.</p> Activation Functions Expression Graph Sigmoid \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\) tanh tanh(x) ReLU(A good default choice for most problems) max(0,x) <p>A simple achievement:</p> Python<pre><code>import numpy as np\nfrom numpy.random import randn\n\nN,Din,H,Dout = 64,1000,100,10\nx,y = randn(N,Din),randn(N,Dout)\nw1,w2 = randn(Din,H),randn(H,Dout)\nfor t in range(10000):\n    h = 1.0 / (1.0 + np.exp(-x.dot(w1)))\n    y_pred = h.dot(w2)\n    loss = np.square(y_pred - y).sum()\n    dy_pred = 2.0 * (y_pred - y)\n    dw2 = h.T.dot(dy_pred)\n    dh = dy_pred.dot(w2.T)\n    dw1 = x.T.dot(dh*h*(1-h))\n    w1 -= 1e-4 * dw1\n    w2 -= 1e-4 * dw2\n</code></pre> <p>Space warping:</p> <p>Linear transform cannot linearly separate points even in feature space.</p> <p>but with ReLU function,</p> <p>Universal Approximation:</p> <p>\u200b   use layer bias to move the graph</p> <p></p> <p>use many ReLU to approach the function.</p> <p>to reach 0 or unchanged: slope should be opposite</p> <p>let coefficient of x be 1,only change the shaping factor of MAX.</p> <p>Convex Functions:</p> <p>\\(f:X \\subset \\mathbb{R}^N \\rightarrow \\mathbb{R}\\) is convex if for all \\(x_1,x_2 \\in X,t\\in[0,1],f(tx_1+(1-t)x_2)\\leq tf(x_1)+(1-t)f(x_2)\\)</p> <p>convex is easy to optimize</p>"},{"location":"notes/EECS498/Optimization/","title":"\u7b2c\u4e8c\u8282(Optimization)","text":"<p>\\(w^* = \\arg \\min_wL(w)\\)</p> <p>Idea #1 :Random Search(Bad Idea!)</p> Python<pre><code>bestloss = float('inf')\nfor num in xrange(1000):\n    W = np.random.randn(10,3073) * 0.001\n    loss = L(X_train,Y_train,W) #L is the loss function\n    if loss &lt; bestloss:\n        bestloss = loss\n        bestW = W\n    print(f'in attempt {num} the loss was {loss},best {bestloss}')\n</code></pre> <p>Batch Gradient Descent</p> <p>\\(L(W) = \\frac{1}{N}\\sum_{i=1}^NL_i(x_i,y_i,W)+\\lambda R(W)\\)</p> <p>\\(\\nabla_WL(W)=\\frac{1}{N}\\sum_{i=1}^N\\nabla_WL_i(x_i,y_i,W)+\\lambda\\nabla_WR(W)\\)</p> <p>Idea #2 : Stochastic Gradient Descent</p> Python<pre><code>w = initialize_weights()\nfor t in range(num_steps):\n    minibatch = sample_data(data,batch_size)\n    dw = compute_gradient(loss_fn,minibatch,w)\n    w- = learning_rate * dw\n</code></pre> <p>SGD: \\(x_{t+1}=x_t - \\alpha \\nabla f(x_t)\\)</p> <p>Problems:1.overshoot and never get back</p> <p>\u200b       2.settle in local minimum and saddle point</p> <p>SGD+Momentum: \\(v_{t+1}=\\rho v_t -\\alpha \\nabla f (x_t)\\)</p> <p>\u200b               \\(x_{t+1}=x_t+v_{t+1}\\) </p> <p>Nesterov Momentum:\\(v_{t+1}=\\rho v_t-\\alpha \\nabla f(x_t+\\rho v_t)\\)</p> <p>\u200b               \\(x_{t+1}=x_t+v_{t+1}\\)   Not that good :Not intuitively clear,because it uses the data of future status</p> <p>\u200b       or            $\\tilde{x_t} =x_t + \\rho v_t $</p> <p>\u200b               \\(v_{t+1}=\\rho v_t -\\alpha \\nabla f (\\tilde{x_t})\\)</p> <p>\u200b               \\(\\widetilde{x_{t+1}}=\\tilde{x_t}-\\rho v_t + (1 + \\rho) v_{t+1}\\)</p> <p>\u200b                   \\(=\\tilde{x_t}+v_{t+1}+\\rho (v_{t+1}-v_t)\\)</p> <p>AdaGrad: Progress along \u201csteep\u201d directions is damped;</p> <p>\u200b       progress along \u201cflat\u201d directions is accelerated.</p> Python<pre><code>grad_squared = 0\nfor t in range(num_steps):\n    dw = compute_gradient(w)\n    grad_squared += dw*dw\n    w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)\n</code></pre> <p>Problem: grad_squared is accumulative so that it will stop before getting to the bottom.(it can get very big)</p> <p>RMSProp: a leaky version of Adaguard</p> Python<pre><code>grad_square = 0\nfor t in range(num_steps):\n    dw = compute_gradient(w)\n    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dw * dw\n    w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)\n</code></pre> <p>Adam: RMSProp + Momentum</p> Python<pre><code>moment1 = 0\nmoment2 = 0\nfor t in range(num_steps):\n    dw = compute_gradient(w)\n    moment1 = beta1 * moment1 + (1-beta1) * dw #Momentum\n    moment2 = beta2 * moment2 + (1-beta2) * dw * dw #RMSProp\n    #moment1_unbias = moment1 / (1 - beta1 ** t)\n    #moment2_unbias = moment2 / (1 - beta2 ** t)\n    w -= learning_rate * moment1 / (moment2.sqrt() + 1e-7)\n    # Problem: when beta2 is approximately 1,momenent.sqrt() in the first several steps can be very small,thus leading to the */moment2.sqrt() very big.\n&lt;div markdown=\"1\" style=\"margin-top: -30px; font-size: 0.75em; opacity: 0.7;\"&gt;\n:material-circle-edit-outline: \u7ea6 234 \u4e2a\u5b57 :fontawesome-solid-code: 34 \u884c\u4ee3\u7801 :material-clock-time-two-outline: \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 2 \u5206\u949f\n&lt;/div&gt;\n    #We need to correct the bias.  \n</code></pre> <p>Adam with beta1 = 0.9,beta2 = 0.999,and learning_rate = 1e-3,5e-4,1e-4 is a great starting point for many models.</p>"},{"location":"notes/Probability/%E8%AE%A8%E8%AE%BA2/","title":"\u7b2c\u4e8c\u6b21\u8ba8\u8bba","text":""},{"location":"notes/Probability/%E8%AE%A8%E8%AE%BA2/#_1","title":"\u95ee\u9898\u6982\u8ff0","text":"<p> \u7ea6 591 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 3 \u5206\u949f</p> <p>\u5047\u8bbe\u67d0\u4eba\u611f\u67d3 COVID\uff0c\u8bb0\u4e3a\u4e8b\u4ef6\\(C\\), \\(A\\):\u201c\u6838\u9178\u68c0\u6d4b\u7ed3\u679c\u4e3a\u9633\u6027\u201d,\\(B\\):\u201c\u6297\u539f\u68c0\u6d4b\u5448\u9633\u6027\u201d\u3002 \u5219\\(P(AB|C)=P(A|C)P(B|C)\\) \u8bf7\u901a\u8fc7\u5b9e\u9645\u80cc\u666f\u89e3\u91ca\u4e0a\u8ff0\u7ed3\u679c\u4e3a\u4ec0\u4e48\u6210\u7acb\u3002</p>"},{"location":"notes/Probability/%E8%AE%A8%E8%AE%BA2/#_2","title":"\u95ee\u9898\u89e3\u51b3","text":"<p>\u6838\u9178\u68c0\u6d4b\u9633\u6027\uff08\\(A\\)\uff09\u548c\u6297\u539f\u68c0\u6d4b\u9633\u6027\uff08\\(B\\)\uff09\u867d\u7136\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f46\u5728\u5df2\u77e5\u67d0\u4eba\u5df2\u7ecf\u611f\u67d3\u4e86 COVID \u540e\uff0c\u4e24\u8005\u68c0\u6d4b\u7ed3\u679c\u7684\u6982\u7387\u4e0d\u4f1a\u4e92\u76f8\u5e72\u6270\u3002 \u5728\u611f\u67d3\u4e86 COVID \u7684\u60c5\u51b5\u4e0b\uff0c\u6838\u9178\u68c0\u6d4b\u9633\u6027\u4e0e\u6297\u539f\u68c0\u6d4b\u9633\u6027\u5404\u81ea\u7684\u6982\u7387\u662f\u7531\u5404\u81ea\u7684\u68c0\u6d4b\u65b9\u6cd5\u51b3\u5b9a\u7684\u3002\u5047\u8bbe\u8fd9\u4e24\u79cd\u68c0\u6d4b\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff08\u6bd4\u5982\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\uff09\u662f\u72ec\u7acb\u7684\uff0c\u4e14\u90fd\u53ea\u4e0e\u611f\u67d3\u72b6\u6001\u76f8\u5173\uff0c\u4e0d\u53d7\u5f7c\u6b64\u5f71\u54cd\u3002</p>"},{"location":"notes/Probability/%E8%AE%A8%E8%AE%BA2/#_3","title":"\u8fdb\u4e00\u6b65\u63a2\u7a76","text":"<p>\u89e3\u91ca\\(P(AB|\\overline{C}) = P(A|\\overline{C})\\cdot P(B|\\overline{C})\\)\u4ee5\u53ca \\(P(AB) = P(A)\\cdot P(B)\\) \u662f\u5426\u6210\u7acb\u3002 \u5148\u6765\u770b\u7b2c\u4e00\u4e2a\u5f0f\u5b50\u3002\u5bb9\u6613\u77e5\u9053\u5176\u8868\u793a\u7684\u610f\u601d\u662f\u5df2\u77e5\u67d0\u4eba\u6ca1\u6709\u611f\u67d3COVID\u65f6\uff0c\\(A\\)\u548c\\(B\\)\u662f\u5426\u6761\u4ef6\u72ec\u7acb\u3002\u7b54\u6848\u662f\u663e\u7136\u7684\uff0c\u7531\u4e0a\u8ff0\u539f\u56e0\uff0c\u8fd9\u4e24\u79cd\u68c0\u6d4b\u65b9\u6cd5\u5728\u5df2\u77e5\u662f\u5426\u611f\u67d3\u7684\u60c5\u51b5\u4e0b\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u3002 \u518d\u6765\u770b\u7b2c\u4e8c\u4e2a\u5f0f\u5b50\u3002\u7b2c\u4e8c\u4e2a\u5f0f\u5b50\u662f\u4e0d\u6210\u7acb\u7684\uff0c\u5728\u672a\u77e5\u662f\u5426\u611f\u67d3\u65f6\uff0c\\(A\\)\u53d1\u751f\u4f1a\u4f7f\\(B\\)\u53d1\u751f\u7684\u53ef\u80fd\u6027\u4e0a\u5347\uff08\u5f71\u54cd\\(B\\)\u53d1\u751f\u7684\u53ef\u80fd\u6027\uff09\uff0c\u540c\u7406\uff0c\\(B\\)\u53d1\u751f\u4e5f\u4f1a\u5f71\u54cd\\(A\\)\u53d1\u751f\u7684\u53ef\u80fd\u6027\u3002\u56e0\u4e3a\u6211\u4eec\u6709\u7406\u7531\u76f8\u4fe1\uff0c\u5f53\u6838\u9178\u68c0\u6d4b\u4e3a\u9633\u6027\u65f6\uff0c\u6297\u539f\u68c0\u6d4b\u4e5f\u5927\u6982\u7387\u4e3a \u9633\u6027\uff08\u5f53\u7136\uff0c\u8fd9\u91cc\u5305\u62ec\u4e86\u5047\u9633\u6027\u7b49\u60c5\u51b5\uff09\u3002\u56e0\u6b64\\(A\\),\\(B\\)\u5e76\u4e0d\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u3002</p>"},{"location":"notes/Probability/%E8%AE%A8%E8%AE%BA2/#2","title":"\u5bf9\u8865\u51452\u7684\u601d\u8003","text":"<p>\u8865\u51452\u4e2d\u5047\u9633\u6027\u3001\u5047\u9634\u6027\u7b49\u6761\u4ef6\u7684\u8865\u5145\u4f7f\u5f97\u7ed3\u679c\u66f4\u52a0\u7cbe\u786e\uff0c\u540c\u65f6\u4e5f\u7528\u5230\u4e86\u6761\u4ef6\u72ec\u7acb\uff0c\u56e0\u4e3a\u5728\u5df2\u77e5\u662f\u5426\u60a3\u75c5\u7684\u60c5\u51b5\u4e0b\uff0c\u8840\u6db2\u548c\u5c3f\u6db2\u68c0\u67e5\u7684\u7ed3\u679c\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u3002\u4f46\u662f\u4e0d\u96be\u4ece\\(1)\\)\u548c\\(2)\\)\u7684\u7ed3\u679c\u4e2d\u770b\u51fa\uff0c\u5982\u679c\u5728\u672a\u77e5\u662f\u5426\u60a3\u75c5\u7684\u60c5\u51b5\u4e0b\uff0c\u4e24\u8005\u7684\u7ed3\u679c\u662f\u4f1a\u76f8\u4e92\u5f71\u54cd\u7684\u3002\u7b80\u8a00\u4e4b\uff0c\u5f53\u4e00\u5f00\u59cb\u53ea\u8fdb\u884c\u8840\u6db2\u68c0\u67e5\u65f6\uff0c\u7531\u4e8e\u4eba\u7fa4\u4e2d\u53d1\u75c5\u7387\u8f83\u4f4e\uff0c\u5373\u4f7f\u4e3a\u9633\u6027\uff0c\u60a3\u75c5\u51e0\u7387\u4e5f\u8f83\u4f4e\u3002\u4f46\u662f\u5f53\u5c3f\u6db2\u68c0\u67e5\u4e5f\u4e3a\u9633\u6027\u65f6\uff0c\u60a3\u75c5\u7684\u51e0\u7387\u4e5f\u4f1a\u63d0\u9ad8\u3002</p>"},{"location":"notes/Probability/%E8%AE%A8%E8%AE%BA3/","title":"\u7b2c\u4e09\u6b21\u8ba8\u8bba","text":"<p> \u7ea6 7 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u8be6\u89c1\u8ba8\u8bba3.pdf by 6ch.</p>"},{"location":"summary/","title":"index","text":""},{"location":"summary/#summaries","title":"Summaries \ud83d\uddd3\ufe0f","text":"\u300e \u0915\u093f\u0928\u094d\u0928\u0930\u093f\u092f \u092e\u092e \u0924\u0923\u094d\u0939\u093e \u300f"},{"location":"summary/%E5%85%B7%E4%BD%93%E8%AE%A1%E5%88%92/","title":"\u89c4\u5212","text":"<p> \u7ea6 46 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p> <p>\u6570\u5b66\u5206\u6790\uff1a\u8ddf\u7740\u8001\u5e08\u4e0a\u8bfe\u8d70 \u7ed3\u5408\u8c22\u60e0\u6c11\u8003\u524d\u7a81\u51fb</p> <p>\uff08\u91cd\u8981\uff09\u7a0b\u5e8f\u8bbe\u8ba1\uff1a\u81ea\u5b66 ACMOJ CS61B</p> <p>\u5927\u5b66\u7269\u7406\uff1a\u81ea\u5b66 </p> <p>\uff08\u91cd\u8981\uff09\u6982\u7387\u7edf\u8ba1\uff1aIntroduction to Probability MIT6.041</p>"},{"location":"summary/%E6%88%91%E7%9A%84%E5%A4%A7%E4%B8%80%E4%B8%8A%E5%AD%A6%E6%9C%9F/","title":"\u590d\u76d8","text":"<p> \u7ea6 9 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4\u4e0d\u5230 1 \u5206\u949f</p>"},{"location":"summary/%E6%88%91%E7%9A%84%E5%A4%A7%E4%B8%80%E4%B8%8A%E5%AD%A6%E6%9C%9F/#_1","title":"\u603b\u7ed3\u6211\u7684\u5927\u4e00\u4e0a\u5b66\u671f","text":""}]}