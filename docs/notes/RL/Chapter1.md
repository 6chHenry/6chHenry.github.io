# 《强化学习的数学原理》第1课-基本概念知识点整理


## 核心实例：网格世界（grid-world）
1. 世界构成：由网格组成，包含3类网格
   - 可进入区域（白色）：机器人可正常移动进入
   - 禁止区域（黄色）：课程中定义为「可进入但会受惩罚」（比「物理不可进入」更通用，可能出现“冒险走禁区域抄近路”的有趣行为）
   - 目标区域（target area）：机器人需抵达的最终目标
   - 边界：机器人无法超越，撞边界后会弹回原状态
2. 机器人规则：仅能在相邻网格（上下左右）移动，不可斜向移动
3. 核心任务：找到从起点到目标区域的「优质路径」，直观判断标准包括：
   - 避免进入禁止区域
   - 减少无意义拐弯（如反复往返）
   - 不尝试超越边界


## 三、强化学习核心基础概念
### （一）状态（State）与状态空间（State Space）
1. 状态（State）
   - 定义：描述智能体（agent）相对于环境的状态（status）
   - 网格世界实例：即机器人的「位置」，用s₁、s₂…s₉表示（每个符号对应二维平面的(x,y)坐标）
   - 扩展场景：复杂问题中，状态还可能包含速度、加速度等额外信息
2. 状态空间（State Space）
   - 定义：所有可能状态的集合（本质是数学中的“集合”，非复杂线性空间）
   - 表示符号：用花体「𝓢」表示，形式为𝓢 = {s₁, s₂, ..., sₙ}（n为状态总数，可从上下文推断范围）

### （二）动作（Action）与动作空间（Action Space）
1. 动作（Action）
   - 定义：智能体在某一状态下可采取的具体行为
   - 网格世界实例：共5种动作，分别为a₁（向上）、a₂（向右）、a₃（向下）、a₄（向左）、a₅（原地不动）
2. 动作空间（Action Space）
   - 定义：某一状态下所有可能动作的集合
   - 表示符号：用花体「𝓐」表示，形式为𝓐(sᵢ)（括号标注sᵢ，强调「动作空间与状态相关」，不同状态的可执行动作可能不同）

### （三）状态转移（State Transition）
1. 定义：智能体在某一状态下采取某一动作后，从当前状态移动到下一状态的过程
2. 网格世界实例：
   - 例1：在s₁状态采取a₂（向右）动作，下一状态为s₂
   - 例2：在s₁状态采取a₁（向上）动作，因撞边界弹回，下一状态仍为s₁
   - 例3：在s₅（禁区域相邻）采取a₂（向右）动作，下一状态为s₆（禁区域，会受惩罚）
3. 表示方式：简单场景可用「表格」表示（行=状态，列=动作，单元格=下一状态），但仅适用于「确定性转移」（无随机因素）

### （四）状态转移概率（State Transition Probability）
1. 定义：用「条件概率」描述状态转移的可能性，可处理「随机性转移」（比表格更通用）
2. 表示符号：p(s' | s, a)，含义为“在状态s下采取动作a，转移到状态s'的概率”
3. 实例：
   - 确定性转移：p(s₂ | s₁, a₂) = 1（s₁采取a₂必到s₂），p(sᵢ | s₁, a₂) = 0（i≠2时概率为0）
   - 随机性转移（如考虑风的影响）：p(s₂ | s₁, a₂) = 0.5，p(s₅ | s₁, a₂) = 0.5（s₁采取a₂，50%到s₂、50%到s₅）

### （五）策略（Policy）

1. 定义：指导智能体“在某一状态下应采取何种动作”的规则，是强化学习的核心独有概念
2. 直观表示：用箭头标注每个状态的推荐动作（如s₁箭头向右，表示推荐a₂动作），基于策略可生成「路径（path）」或「轨迹（trajectory）」（如从起点按策略动作依次转移到目标的过程）
3. 数学表示：用「条件概率」π(a | s)描述，含义为“在状态s下采取动作a的概率”，满足「某一状态下所有动作概率和为1」
4. 分类：
   - 确定性策略：某一动作概率为1，其余为0（如π(a₂ | s₁) = 1，π(a₁,a₃,a₄,a₅ | s₁) = 0）
   - 随机性策略：多个动作有非零概率（如π(a₂ | s₁) = 0.5，π(a₃ | s₁) = 0.5，其余为0）
5. 工程实现：
   - 表示：用数组/矩阵存储（行=状态，列=动作，单元格=动作概率）
   - 执行（随机性策略）：从[0,1]均匀随机采样x，按概率区间选择动作（如x∈[0,0.5]选a₂，x∈[0.5,1]选a₃）

### （六）Reward（奖励）

1. **定义与本质**
   - 本质：智能体（agent）在某一状态采取动作后获得的「实数标量」，是人机交互的核心接口（引导智能体行为）
   - 含义：正数表示「鼓励该行为」，负数表示「惩罚该行为」，0表示「无惩罚（一定程度上隐含鼓励）」
   - 灵活性：可通过数学转换调整正负含义（如用正数表惩罚、负数表鼓励），核心是智能体目标与奖励方向一致（最大化鼓励/最小化惩罚）

2. **网格世界实例设计**
   - 边界惩罚：试图超越边界（如s1采取a1向上），奖励`r_bound = -1`
   - 禁区域惩罚：进入禁止区域（如s5采取a2向右到s6），奖励`r_forbid = -1`
   - 目标奖励：进入目标区域（如到s9），奖励`r_target = +1`
   - 其他行为：正常移动（如s1采取a2向右到s2）、原地不动，奖励均为0

3. **关键注意点**
   - 依赖对象：Reward仅依赖「当前状态+当前动作」，与「下一个状态」无关
     - 实例：s1采取a1（撞边界，下状态s1，奖励-1）与s1采取a5（原地不动，下状态s1，奖励0），下状态相同但奖励不同，因动作含义不同
   - 表示方式：
     - 表格形式：行=状态，列=动作，单元格=奖励，仅适用于「确定性奖励」（某状态-动作对应固定奖励）
     - 概率形式：用条件概率`p(r | s, a)`描述，可处理「随机性奖励」（如努力学习可能获得不同正奖励），例：`p(r=-1 | s1, a1) = 1`（确定性）、`p(r=2 | s, a)=0.6, p(r=1 | s, a)=0.4`（随机性）

### （六）Trajectory（轨迹）

1. **定义**：智能体与环境交互形成的「状态-动作-奖励」序列链，即`s₀ →(a₀,r₀) s₁ →(a₁,r₁) s₂ →…→sₙ`
2. **网格世界实例**：从s1出发，s1（a2,0）→s2（a3,0）→s5（a3,0）→s8（a2,1）→s9，形成完整轨迹
3. **属性**：轨迹长度可有限（如到目标后停止）或无限（如目标区域持续执行策略）

### （七）Return（回报）

1. **定义**：某条轨迹上所有奖励的「总和」，是评估轨迹/对应策略好坏的核心指标
   - 公式（有限轨迹）：`Return = r₀ + r₁ + r₂ + … + rₙ`
2. **网格世界实例对比**
   - 优质轨迹（避禁区域）：s1→s2→s5→s8→s9，奖励序列[0,0,0,1]，Return=1
   - 较差轨迹（进禁区域）：s1→s4→s5→s6（禁区域，-1）→s9，奖励序列[0,0,-1,1]，Return=0
   - 结论：Return越大，轨迹/策略越优，实现「直观判断→数学量化」的转换

3. **Discounted Return（折扣回报）**
1. **核心问题**：无限轨迹（如目标区域持续获得+1奖励）的普通Return会「发散到无穷」，无法量化评估
2. **定义**：引入折扣率`γ`（`0 ≤ γ ≤ 1`），对未来奖励按「时间步次方」衰减，公式为：
   - `Discounted Return = r₀ + γr₁ + γ²r₂ + γ³r₃ + …`
3. **折扣率`γ`的作用**
   - 收敛性：当`γ < 1`时，无限奖励序列和收敛为有限值（等比数列求和：`S = r₀ + γr₁ + … = r₀ + γ*(r₁ + γr₂ + …)`，若后续奖励恒定为r，总和为`r/(1-γ)`）
   - 行为引导：
     - `γ→0`：智能体「近视」，仅关注当前/近期奖励（如`γ=0.1`，未来3步奖励衰减至0.001，影响可忽略）
     - `γ→1`：智能体「远视」，重视长远奖励（如`γ=0.99`，未来30步奖励仍有`0.99³⁰≈0.74`，影响显著）

4. **网格世界实例计算**：目标区域持续获得+1奖励，轨迹后半段为s9（a5,1）→s9（a5,1）→…，折扣回报为：
   - 假设前3步奖励为0，从第4步开始为1：`0 + 0 + 0 + γ³*1 + γ⁴*1 + … = γ³/(1-γ)`（`γ < 1`时收敛）

### （八）Episode（回合）

- 定义：有「终止状态（terminal state）」的有限轨迹，即智能体从初始状态出发，到终止状态后停止，形成一个回合（如游戏通关、机器人到达目标后停止）
- 对应任务：Episodic Tasks（回合制任务），例：网格世界到达目标后停止

1. **Continuing Tasks（持续型任务）**
   - 定义：无终止状态，智能体与环境交互「永久持续」的任务（现实中无绝对永久，需近似），例：机器人长期巡逻、持续优化的推荐系统

2. **两种任务的统一方法**
   - 方法1：将终止状态设为「吸收状态（absorbing state）」
     - 规则：进入吸收状态后，无论采取何种动作，均停留在该状态，且后续奖励全为0（如目标区域仅允许原地不动，奖励0）
   - 方法2：将终止状态视为「普通状态」
     - 规则：目标区域有正常策略（如策略优则持续停留获+1，策略差则可能跳出），无需特殊处理，更具一般性（课程采用此方法）

## 核心框架：Markov Decision Process（MDP，马尔可夫决策过程）

### （一）MDP的核心要素
MDP是强化学习的统一数学框架，包含5个核心要素，记为`MDP = (𝓢, 𝓐, 𝓡, p, π)`，各要素含义如下：
| 要素符号 | 要素名称 | 定义与说明                                                   |
| -------- | -------- | ------------------------------------------------------------ |
| 𝓢        | 状态空间 | 所有可能状态的集合，如网格世界中𝓢 = {s₁, s₂, …, s₉}          |
| 𝓐        | 动作空间 | 每个状态对应的可执行动作集合，与状态相关，记为𝓐(s)，如𝓐(s₁) = {a₁,a₂,a₃,a₄,a₅} |
| 𝓡        | 奖励空间 | 所有可能奖励的集合，如网格世界中𝓡 = {-1, 0, +1}              |
| p        | 概率分布 | 包含状态转移概率`p(s' | s, a)`和奖励概率`p(r | s, a)`，描述环境随机性 |
| π        | 策略     | 状态到动作的映射，用`π(a | s)`表示状态s下采取动作a的概率，指导智能体行为 |

### （二）MDP的核心性质：Markov Property（马尔可夫性质）
1. **定义**：「无记忆性」——智能体下一个状态的概率仅依赖「当前状态+当前动作」，与「历史状态/动作」无关，公式表达为：
   - 状态转移：`p(sₜ₊₁ | sₜ, aₜ) = p(sₜ₊₁ | s₀,a₀,s₁,a₁,…,sₜ,aₜ)`
   - 奖励：`p(rₜ | sₜ, aₜ) = p(rₜ | s₀,a₀,s₁,a₁,…,sₜ,aₜ)`
2. **含义**：简化问题复杂度，无需存储历史交互信息，仅需关注当前状态与动作即可预测未来

### （三）MDP与Markov Process（马尔可夫过程）的关系
- Markov Process（MP）：仅包含「状态空间𝓢」和「状态转移概率p」，无动作与策略，是无决策的状态演化过程
- MDP与MP的转化：当MDP中的策略π固定时，策略与环境融合，动作选择由π确定，MDP退化为MP（即固定策略下的状态转移过程）
- 实例：网格世界中固定策略π（s1→a2, s2→a3,…），状态转移仅依赖当前状态与π，形成MP
