# 贝尔曼公式（强化学习）核心知识点总结
## 一、贝尔曼公式基础认知

1. **核心定义**：描述不同状态的**状态价值（State Value，记为 $v_\pi(s)$）** 之间的关系，是计算状态价值的核心工具。
2. **本质逻辑**：将当前状态的价值拆分为“即时奖励”和“未来状态价值的折现”两部分，体现强化学习中“短期收益”与“长期收益”的权衡。


## 二、推导前置概念与公式
1. **关键术语回顾**
   - **折扣回报（Discounted Return，$G_t$）**：从时刻$t$开始的累计奖励，需考虑折扣因子$\gamma$（$0 \leq \gamma \leq 1$），公式为：$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$
   - **状态价值（State Value，$v_\pi(s)$）**：在策略$\pi$下，从状态$s$出发的折扣回报的期望，即$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$
2. **回报拆分关键步骤**：将$G_t$拆分为即时奖励和未来回报的折现，即$G_t = R_{t+1} + \gamma G_{t+1}$，为后续期望拆分奠定基础。


## 三、贝尔曼公式推导过程
1. **第一步：拆分期望（基于回报拆分）**  
   对状态价值公式中的期望进行拆分（期望的线性性质）：  
   $v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] = \mathbb{E}_\pi[R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]$ 
   拆分后得到两项：**即时奖励的期望**和**未来回报期望的折现**。

2. **第二步：计算"即时奖励的期望"**  
   需考虑策略$\pi$（状态$s$下选择动作$a$的概率$\pi(a|s)$）和环境动态（动作$a$下获得奖励$r$的概率$p(r|s,a)$），公式为：  
   $\mathbb{E}_\pi[R_{t+1} | S_t = s] = \sum_a \pi(a|s) \mathbb{E}[R_{t+1}|S_t=s,A_t=a] \\ =\sum_a \pi(a|s) \sum_r r \cdot p(r|s,a)$
   本质是“策略下所有动作的期望奖励加权和”。

3. **第三步：计算"未来回报期望的折现"**  

   - 利用**马尔可夫性质（无记忆性）**：未来回报仅依赖于下一个状态$s'$，与当前状态$s$无关，即$\mathbb{E}_\pi[G_{t+1} | S_t = s] = \mathbb{E}_\pi[v_\pi(S_{t+1}) | S_t = s]$。  

   - 结合环境动态（动作$a$下转移到状态$S_{t+1}s'$的概率$p(s'|s,a)$），公式为：  

     $
     \begin{align*}
     \mathbb{E}[G_{t+1}|S_t = s] &= \sum_{s'} \mathbb{E}[G_{t+1}|S_t = s, S_{t+1} = s']p(s'|s) \\
     &= \sum_{s'} \mathbb{E}[G_{t+1}|S_{t+1} = s']p(s'|s) \\
     &= \sum_{s'} v_\pi(s')p(s'|s) \\
     &= \sum_{s'} v_\pi(s') \sum_{a} p(s'|s,a)\pi(a|s) \\ &=\sum_a\pi(a|s)\sum_{s'}p(s'|s,a)v_\pi(s')
     \end{align*}
     $

4. **最终贝尔曼公式**  
   合并上述两项，得到完整公式：  
   $v_\pi(s) = \sum_a \pi(a|s) \left[ \sum_r r \cdot p(r|s,a) + \gamma \sum_{s'} p(s'|s,a) \cdot v_\pi(s') \right]$


## 四、贝尔曼公式的关键特性
1. **多方程性**：并非单一公式，而是对**状态空间中所有状态$s$** 都成立。若有$n$个状态，会形成$n$个线性方程，联立可解出所有状态价值。
2. **策略依赖性**：公式中的$\pi(a|s)$（策略）决定了状态价值的计算结果，不同策略对应不同的状态价值（核心用途：**策略评估（Policy Evaluation）**，即判断策略优劣）。
3. **环境动态依赖性**：依赖$p(r|s,a)$和$p(s'|s,a)$（环境模型/动态模型），若已知模型可直接计算；若未知模型，需用无模型强化学习（如蒙特卡洛、时序差分）方法。
4. **自举性（Bootstrapping）**：当前状态价值的计算依赖其他状态价值，需通过联立方程或迭代方法求解（而非直接计算）。

## 五、实例应用（基于视频案例）

### 案例1：单动作、确定转移的简单场景
- **设定**：状态$s_1$下，策略$\pi(a_3|s_1)=1$（仅选动作$a_3$）；动作$a_3$的奖励$r=0$（概率1），转移到$s_3$（概率1）。
- **代入公式**：  
  即时奖励期望=0，未来回报期望=$v_\pi(s_3)$，最终贝尔曼公式简化为：$v_\pi(s_1) = 0 + \gamma \cdot v_\pi(s_3)$。
- **直观结论**：$s_1$的价值仅依赖$s_3$的价值，符合"离目标状态越近，价值越高"的逻辑（如视频中$s_2/s_3/s_4$价值为10，$s_1$价值为$9=\gamma \times 10$，$\gamma=0.9$）。

### 案例2：多动作、概率转移的场景
- **设定**：状态$s_1$下，策略$\pi(右|s_1)=0.5$、$\pi(下|s_1)=0.5$；向右转移到$s_2$（奖励-1），向下转移到$s_3$（奖励0）。
- **代入公式**：  
  $v_\pi(s_1) = 0.5 \times (-1 + \gamma v_\pi(s_2)) + 0.5 \times (0 + \gamma v_\pi(s_3))$。
- **策略对比**：该策略下$s_1$价值为8.9（$\gamma=0.9$），低于案例1中$s_1=9$的价值，说明"含惩罚动作的策略更差"。


## 六、公式的延伸意义
1. **后续应用**：通过贝尔曼公式计算出状态价值后，可进一步**改进策略**（如选择能提升状态价值的动作），最终逼近最优策略。
2. **与无模型方法的关联**：若未知环境模型（$p(r|s,a)$、$p(s'|s,a)$），贝尔曼公式的思想仍适用（如时序差分方法用经验估计期望），是强化学习的核心理论基础。

# 第2课-贝尔曼公式（公式向量形式与求解）知识点整理
## 一、贝尔曼公式的矩阵向量形式推导
### 1. 推导前提
- 单条贝尔曼公式无法求解状态价值（State Value），因公式中同时包含当前状态价值与其他状态价值。
- 对所有状态（共n个）而言，针对单个状态的贝尔曼公式（Elementwise Form）均成立，可通过整合n条公式得到矩阵向量形式，便于求解与理解。

### 2. 核心公式演化
- **原始贝尔曼公式简化**：将公式中“即时奖励项”与“状态转移概率项”合并，得到简化式：  
  $v^\pi(s) = r^\pi(s) + \gamma \sum_{s'} P^\pi(s \to s') v^\pi(s')$  
  - $r^\pi(s)$：从当前状态$s$出发，即时奖励（Immediate Reward）的平均值。  
  - $P^\pi(s \to s')$：从状态$s$转移到状态$s'$的概率（基于策略$\pi$）。
- **状态编号与向量定义**：对所有状态标记为$s_1, s_2, ..., s_n$，定义以下向量与矩阵：
  - **状态价值向量** $v^\pi$：$v^\pi = \begin{bmatrix} v^\pi(s_1) \\ v^\pi(s_2) \\ ... \\ v^\pi(s_n) \end{bmatrix}$，包含所有状态的价值。
  - **即时奖励向量** $r^\pi$：$r^\pi = \begin{bmatrix} r^\pi(s_1) \\ r^\pi(s_2) \\ ... \\ r^\pi(s_n) \end{bmatrix}$，包含所有状态的即时奖励平均值。
  - **状态转移矩阵** $P^\pi$（State Transition Matrix）：  
    矩阵元素$[P^\pi]_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，即$[P^\pi]_{ij} = P^\pi(s_i \to s_j)$。
- **最终矩阵向量形式**：$v^\pi = r^\pi + \gamma P^\pi v^\pi$

### 3. 实例说明
#### 实例1：确定性策略（4个状态）
- **即时奖励向量$r^\pi$**：  
  从$s_1$出发即时奖励为0，从$s_2, s_3, s_4$出发即时奖励均为1，故$r^\pi = \begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \end{bmatrix}$。
- **状态转移矩阵$P^\pi$**：  
  第一行对应$s_1$的转移概率：$s_1 \to s_1$（0）、$s_1 \to s_2$（0）、$s_1 \to s_3$（1）、$s_1 \to s_4$（0），即第一行为$[0, 0, 1, 0]$，其余行按策略同理推导。

#### 实例2：概率性策略（4个状态）
- **即时奖励向量$r^\pi$（以$s_1$为例）**：  
  $s_1$有0.5概率向右转移（奖励-1）、0.5概率向下转移（奖励0），故$r^\pi(s_1) = 0.5 \times (-1) + 0.5 \times 0 = -0.5$。
- **状态转移矩阵$P^\pi$（以$s_1$为例）**：  
  $s_1 \to s_2$（0.5）、$s_1 \to s_3$（0.5）、$s_1 \to s_1$（0）、$s_1 \to s_4$（0），即第一行为$[0, 0.5, 0.5, 0]$。


## 二、状态价值（$v^\pi$）的求解方法
### 1. 求解的核心意义
- 该过程称为**策略评估（Policy Evaluation）**，是强化学习的基础工具：只有通过评估策略的状态价值，才能判断策略优劣，进而改进策略、找到最优策略。

### 2. 两种求解方法
#### 方法1：解析解（Closed-Form Solution）
- **公式推导**：基于矩阵向量形式$v^\pi = r^\pi + \gamma P^\pi v^\pi$，移项整理得：  
  $(I - \gamma P^\pi) v^\pi = r^\pi$（$I$为单位矩阵），两边左乘$(I - \gamma P^\pi)^{-1}$，最终解析解为：  
  $v^\pi = (I - \gamma P^\pi)^{-1} r^\pi$。
- **局限性**：仅理论优美，实际中因状态空间大时矩阵维度高，求逆计算量极大，难以应用。

#### 方法2：迭代法（Iterative Solution）
- **核心思想**：通过迭代更新状态价值向量，直至收敛到真实值$v^\pi$。
- **迭代公式**：$v_{k+1}^\pi = r^\pi + \gamma P^\pi v_k^\pi$  
  - $v_k^\pi$：第$k$次迭代的状态价值向量（初始可设为全0等任意值）。  
  - $v_{k+1}^\pi$：第$k+1$次迭代的状态价值向量，由$v_k^\pi$代入公式计算得到。
- **收敛性**：可证明当迭代次数$k \to \infty$时，$v_k^\pi$收敛到真实状态价值$v^\pi$（通过定义误差$\delta_k = \| v_k^\pi - v^\pi \|$，可证$\delta_k \to 0$）。

>  Proof. Define the error as $\delta_k = v_k - v_\pi$. We only need to show $\delta_k \to 0$. 
>
> Substituting $v_{k+1} = \delta_{k+1} + v_\pi$ and $v_k = \delta_k + v_\pi$ into $v_{k+1} = r_\pi + \gamma P_\pi v_k$ gives $$ \delta_{k+1} + v_\pi = r_\pi + \gamma P_\pi (\delta_k + v_\pi), $$ 
>
> which can be rewritten as
>
>  $$ \delta_{k+1} = -v_\pi + r_\pi + \gamma P_\pi \delta_k + \gamma P_\pi v_\pi = \gamma P_\pi \delta_k. $$ 
>
> As a result, $$ \delta_{k+1} = \gamma P_\pi \delta_k = \gamma^2 P_\pi^2 \delta_{k-1} = \dots = \gamma^{k+1} P_\pi^{k+1} \delta_0. $$ 
>
> Note that $0 \leq P_\pi^k \leq 1$, which means every entry of $P_\pi^k$ is no greater than 1 for any $k = 0,1,2,\dots$. 
>
> That is because $P_\pi^k \mathbf{1} = \mathbf{1}$, where $\mathbf{1} = [1,\dots,1]^T$. 
>
> On the other hand, since $\gamma < 1$, we know $\gamma^k \to 0$ and hence $\delta_{k+1} = \gamma^{k+1} P_\pi^{k+1} \delta_0 \to 0$ as $k \to \infty$.

1. **定义误差项**：
   首先定义误差 $\delta_k = v_k - v_\pi$，这里 $v_k$ 是第 $k$ 次迭代的状态价值向量，$v_\pi$ 是策略 $\pi$ 对应的真实状态价值向量。我们的目标是证明当迭代次数 $k$ 趋于无穷时，$\delta_k$ 趋于 $0$，即迭代得到的 $v_k$ 收敛到真实的 $v_\pi$。
2. **代入迭代公式**：
   已知迭代公式为 $v_{k+1} = r_\pi + \gamma P_\pi v_k$（其中 $r_\pi$ 是即时奖励向量，$P_\pi$ 是状态转移矩阵，$\gamma$ 是折扣因子）。将 $v_{k+1} = \delta_{k+1} + v_\pi$ 和 $v_k = \delta_k + v_\pi$ 代入该迭代公式，得到：
   $$\delta_{k+1} + v_\pi = r_\pi + \gamma P_\pi (\delta_k + v_\pi)$$
3. **化简得到误差递推关系**：
   对上述等式进行整理，将含 $v_\pi$ 的项移到等式一边：
   $$\delta_{k+1} = -v_\pi + r_\pi + \gamma P_\pi \delta_k + \gamma P_\pi v_\pi$$
   又因为真实的状态价值向量 $v_\pi$ 满足贝尔曼方程 $v_\pi = r_\pi + \gamma P_\pi v_\pi$，即 $-v_\pi + r_\pi + \gamma P_\pi v_\pi = 0$，所以上式可化简为：
   $$\delta_{k+1} = \gamma P_\pi \delta_k$$
4. **递推展开误差项**：
   由 $\delta_{k+1} = \gamma P_\pi \delta_k$，可以不断递推得到：
   $$\delta_{k+1} = \gamma P_\pi \delta_k = \gamma^2 P_\pi^2 \delta_{k - 1} = \dots = \gamma^{k + 1} P_\pi^{k + 1} \delta_0$$
   这里 $\delta_0 = v_0 - v_\pi$ 是初始误差（$v_0$ 为初始的状态价值向量）。
5. **分析收敛性**：
   - 对于状态转移矩阵的幂 $P_\pi^k$，由于状态转移概率都是非负的，且每行的和为 $1$（因为从一个状态转移到所有可能状态的概率和为 $1$），所以 $P_\pi^k$ 的每个元素都满足 $0 \leq [P_\pi^k]_{ij} \leq 1$。并且有 $P_\pi^k \mathbf{1} = \mathbf{1}$（其中 $\mathbf{1}$ 是全 $1$ 向量），这也验证了 $P_\pi^k$ 元素的有界性。
   - 已知折扣因子 $\gamma < 1$，所以当 $k \to \infty$ 时，$\gamma^{k + 1} \to 0$。
   - 结合 $P_\pi^{k + 1}$ 元素的有界性（每个元素都不超过 $1$），以及初始误差 $\delta_0$ 是有限向量，可得 $\gamma^{k + 1} P_\pi^{k + 1} \delta_0 \to 0$，即 $\delta_{k + 1} \to 0$（当 $k \to \infty$ 时）。
   因此，迭代得到的状态价值向量 $v_k$ 收敛到真实的状态价值向量 $v_\pi$，从而证明了用迭代法求解状态价值的收敛性。


## 三、策略评估的实例与结论
### 1. 实例背景
- 奖励规则：尝试跳过边界/进入禁止区域（Forbidden Area）得-1，进入目标区域（Target Area）得+1，其他情况按策略定。
- 折扣因子$\gamma = 0.9$。

### 2. 不同策略的评估结果
#### （1）优质策略
- **策略特征**：不跳边界、不撞墙、不进禁止区域，能导向目标区域。
- **状态价值特征**：所有状态价值均为正数；且**离目标区域越近，状态价值越大**（符合直觉，近目标区域更易获得高奖励）。
- **特殊结论**：不同策略可能得到相同状态价值（如某两个格子分别“向下走”和“向右走”，最终价值一致）。

#### （2）劣质策略
- **策略1**：所有状态均“向右走”（易撞墙/跳边界），状态价值全为负数。
- **策略2**：随机策略（多状态会撞墙/进禁止区域），状态价值与直觉一致（整体偏低）。

### 3. 核心结论
通过计算状态价值$v^\pi$，可直观判断策略的优劣：优质策略对应正的、较高的状态价值，劣质策略对应负的、较低的状态价值。


## 四、关键术语汇总
| 术语         | 英文                               | 核心定义                                            |
| ------------ | ---------------------------------- | --------------------------------------------------- |
| 状态价值     | State Value（$v^\pi$）             | 从某状态出发，基于策略$\pi$的长期奖励期望           |
| 即时奖励向量 | Immediate Reward Vector（$r^\pi$） | 所有状态的即时奖励平均值构成的向量                  |
| 状态转移矩阵 | State Transition Matrix（$P^\pi$） | 元素$[P^\pi]_{ij}$表示从$s_i$到$s_j$的转移概率      |
| 策略评估     | Policy Evaluation                  | 基于贝尔曼公式求解$v^\pi$，判断策略优劣的过程       |
| 解析解       | Closed-Form Solution               | 直接通过矩阵求逆得到的$v^\pi$解析表达式，计算成本高 |
| 迭代法       | Iterative Solution                 | 通过迭代更新$v^\pi$直至收敛，实际应用中常用         |

# 第2课-贝尔曼公式（Action Value的定义）知识点总结
## 核心概念：Action Value（动作价值）

### 1. 定义
- **本质**：智能体（Agent）从当前状态$s$出发，选择某个动作$a$后，后续遵循策略$π$所能获得的**平均回报（Average Return）**。
- **符号表示**：$q_π(s,a)$，其中：
  - $π$：表示当前遵循的策略，不同策略对应不同的动作价值；
  - $(s,a)$：状态-动作对，动作价值依赖于“从哪个状态出发”和“选择哪个动作”，是状态与动作的二元函数。


### 2. 核心作用
- 动作价值是**策略选择的核心依据**：在某一状态下，智能体通过比较不同动作的$q_π(s,a)$，优先选择价值更大的动作（该动作能带来更多长期回报）；
- 后续强化学习算法（如Sarsa、Q-Learning）均以动作价值为核心展开，是实现策略优化的关键。

## Action Value与State Value（状态价值）的关系

### 1. 两者区别与联系
| 维度         | State Value（状态价值）$v_π(s)$      | Action Value（动作价值）$q_π(s,a)$                  |
| ------------ | ------------------------------------ | --------------------------------------------------- |
| **定义**     | 从状态$s$出发，遵循策略$π$的平均回报 | 从状态$s$出发、选择动作$a$后，遵循策略$π$的平均回报 |
| **依赖对象** | 仅依赖状态$s$和策略$π$               | 依赖状态$s$、动作$a$和策略$π$                       |
| **核心作用** | 评估状态的“好坏”                     | 评估“状态-动作对”的“好坏”，直接指导动作选择         |

### 2. 数学关联（核心公式）
#### （1）由Action Value推导State Value
状态价值是该状态下所有可能动作的动作价值，按**策略$π$的动作选择概率**加权平均的结果：  
$v_π(s) = Σ_a [π(a|s) * q_π(s,a)]$  

- $π(a|s)$：策略$π$下，在状态$s$选择动作$a$的概率；
- 含义：状态的价值由“该状态下所有可能动作的价值”及其“被选择的概率”共同决定。

#### （2）由State Value推导Action Value
结合贝尔曼公式（状态价值的递归表达式），可推导出动作价值的表达式：  
$q_π(s,a) = E[R_{t+1} + γ * v_π(S_{t+1}) | S_t=s, A_t=a] = \sum_r r \cdot p(r|s,a) + \gamma \sum_{s'} p(s'|s,a) \cdot v_\pi(s')$  

- $R_{t+1}$：选择动作$a$后获得的即时奖励；
- $γ$：折扣因子（衡量未来回报的重要性）；
- $S_{t+1}$：执行动作$a$后转移到的下一状态；
- 含义：动作价值 = 即时奖励 + 折扣后的下一状态价值的期望；
- 两者是“硬币的两面”：已知所有动作价值可求状态价值，反之已知所有状态价值也可求动作价值。

## 关键易错点与实例解析

### 1. 易错点：未被策略选择的动作，其Action Value是否为0？
- **结论**：否。即使策略$π$在状态$s$下仅选择某一动作（如仅选$a2$），其他动作（如$a1$、$a3$）的动作价值仍需计算，而非0；
- **原因**：后续策略优化（如策略改进）需比较所有动作的价值，可能发现未被选择的动作实际价值更高，进而更新策略（选择该动作）。

### 2. 实例计算（确定性策略场景）
假设场景：状态$s1$有动作$a2$（策略指定动作）和$a3$（未被策略选择），转移规则如下：
- 选择$a2$：即时奖励$R=-1$，转移到$s2$，$v_π(s2)$已知；
- 选择$a3$：即时奖励$R=0$，转移到$s3$，$v_π(s3)$已知。

则动作价值计算为：
- $q_π(s1,a2) = -1 + γ * v_π(s2)$（确定性转移，期望即确定值）；
- $q_π(s1,a3) = 0 + γ * v_π(s3)$（即使未被策略选择，仍需按公式计算）。

## Action Value的计算方法

1. **间接法**：先通过贝尔曼公式求解所有状态的$v_π(s)$，再代入动作价值公式$q_π(s,a) = E[R_{t+1} + γ * v_π(S_{t+1})]$计算；
2. **直接法**：不依赖状态价值，直接通过数据（如轨迹采样）估算动作价值（后续蒙特卡洛、时序差分方法会详细讲解）；
3. 两种方法均支持“基于模型”（已知状态转移概率和奖励函数）和“无模型”（未知环境模型，依赖采样数据）场景。

---

总结

- **State value**: $ v_{\pi}(s) = \mathbb{E}[G_t | S_t = s] $
- **Action value**: $q_{\pi}(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a] $
- **The Bellman equation (elementwise form)**:
  $
  \begin{align*}
  v_{\pi}(s) &= \sum_{a} \pi(a|s) \underbrace{\left[ \sum_{r} p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a)v_{\pi}(s') \right]}_{q_{\pi}(s, a)} \\
  &= \sum_{a} \pi(a|s) q_{\pi}(s, a)
  \end{align*}
  $
- **The Bellman equation (matrix-vector form)**:
  $
  v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi}
  $