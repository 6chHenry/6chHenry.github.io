# 《强化学习的数学原理》第2课-贝尔曼公式（知识点整理

## Return的核心作用：策略评估的量化工具

### （一）Return概念复习
- 定义：某条轨迹上所有奖励的“折扣总和”，公式为`Return = r₀ + γr₁ + γ²r₂ + …`（`γ`为折扣率，`0≤γ≤1`）
- 本质：将“轨迹的优劣”转化为可计算的数值，是连接“直观判断”与“数学分析”的纽带

### （二）实例验证：3种策略的Return对比

1. **实验设定**
   - 环境：统一的网格世界（含目标区域、禁止区域、可通行区域）
   - 变量：仅s1状态的策略不同，其他状态策略一致
     - 策略1：s1→向下走（无禁止区域风险）
     - 策略2：s1→向右走（必进禁止区域）
     - 策略3：s1→50%概率向右、50%概率向下（随机风险）

2. **各策略Return计算（基于折扣回报）**
   | 策略  | 轨迹特点               | Return计算过程                                               | 最终结果       |
   | ----- | ---------------------- | ------------------------------------------------------------ | -------------- |
   | 策略1 | 避禁止区域，直达目标   | 轨迹：s1→s3→s4（目标），后续持续获+1奖励，Return=γ*(1 + γ + γ² + …) = γ/(1-γ) | γ/(1-γ)        |
   | 策略2 | 必进禁止区域，再到目标 | 轨迹：s1→禁止区域（-1）→s4（目标），后续持续获+1奖励，Return=-1 + γ/(1-γ) | -1 + γ/(1-γ)   |
   | 策略3 | 随机轨迹（50%风险）    | 按概率加权：0.5*（策略2 Return） + 0.5*（策略1 Return）      | -0.5 + γ/(1-γ) |

3. **结论：Return实现策略量化评估**
   - 数学关系：`Return1 > Return3 > Return2`
   - 直观对应：策略1最优（无风险）→策略3居中（随机风险）→策略2最差（必进禁止区域）
   - 核心价值：Return将“策略好坏”从“主观判断”转化为“客观数值比较”，为后续策略改进提供量化依据

## Return的计算方法与Bootstrapping思想

### （一）两种计算方法对比
#### 方法1：基于定义的直接计算
- 原理：按轨迹顺序，逐次累加“折扣奖励”
- 示例（4状态循环系统，s1→s2→s3→s4→s1）：
  - s1的Return（记为v1）：`v1 = r1 + γr2 + γ²r3 + γ³r4 + γ⁴r1 + …`
  - s2的Return（记为v2）：`v2 = r2 + γr3 + γ²r4 + γ³r1 + …`
- 缺点：需遍历完整轨迹（无限轨迹时计算复杂）

#### 方法2：基于状态价值依赖的递推计算
- 原理：利用“当前状态价值 = 即时奖励 + 折扣×下一个状态价值”的递推关系
- 示例（同4状态系统）：
  - `v1 = r1 + γv2`（s1的价值=即时奖励r1 + 折扣后s2的价值）
  - `v2 = r2 + γv3`（s2的价值=即时奖励r2 + 折扣后s3的价值）
  - `v3 = r3 + γv4`，`v4 = r4 + γv1`
- 优势：无需遍历轨迹，通过状态间的依赖关系简化计算

### （二）Bootstrapping（自举）思想
1. **定义**：通过“待求状态价值之间的依赖关系”求解自身的思想，即“用状态价值的集合求解该集合中的每个元素”
2. **直观类比**：类似“拉着鞋带试图把自己提起来”，看似矛盾，实则通过数学转化可解
3. **核心意义**：为贝尔曼公式的推导提供核心思路——将无限轨迹的Return计算转化为有限的线性方程组求解

## 贝尔曼公式的雏形：矩阵向量形式与求解

### （一）矩阵向量形式推导
1. **变量定义**
   - 状态价值向量：`v = [v1, v2, v3, v4]^T`（4个状态的价值）
   - 即时奖励向量：`r = [r1, r2, r3, r4]^T`（各状态的即时奖励）
   - 状态转移矩阵：`P`（`P[i][j]`表示从状态i转移到状态j的概率，示例中为循环转移，`P = [[0,1,0,0],[0,0,1,0],[0,0,0,1],[1,0,0,0]]`）
2. **公式转化**
   - 递推关系：`v = r + γPv`（状态价值 = 即时奖励 + 折扣×状态转移×状态价值）
   - 本质：这是“确定性策略+确定性转移”场景下的贝尔曼公式雏形

### （二）求解方法（线性代数）

1. **公式变形**：将含v的项移到左侧，得`(I - γP)v = r`（I为单位矩阵）
2. **求解条件**：当`||γP|| < 1`（γ<1且P为转移矩阵）时，`(I - γP)`可逆
3. **解的形式**：`v = (I - γP)⁻¹r`（通过矩阵求逆即可得到各状态的价值）
4. **核心启示**：矩阵向量形式将“多个递推方程”整合为“单一线性方程组”，大幅简化求解过程，为后续一般化贝尔曼公式奠定基础

## 实例应用：网格世界中的贝尔曼公式

1. **场景回归**：基于第1课的网格世界（s1-s9，含目标、禁止区域）
2. **贝尔曼公式书写（以特定策略为例）**
   - s1（向下到s3，即时奖励0）：`v1 = 0 + γv3`
   - s2（向右到s4，即时奖励1）：`v2 = 1 + γv4`
   - s3（向右到s4，即时奖励0）：`v3 = 0 + γv4`
   - ...（其他状态同理，按策略确定转移方向与即时奖励）
3. **求解方式**：上述公式构成线性方程组，可通过手算（小规模）或程序（大规模）求解，直接得到各状态的价值，量化不同状态的优劣

## 前置基础：核心符号与单步交互过程

1. **核心符号定义**
   - **状态（State）**：`S_t`（大写，代表随机变量），表示时刻`t`的状态；`s`（小写，代表具体取值），表示状态的某个确定值。
   - **动作（Action）**：`A_t`（大写，随机变量），表示时刻`t`采取的动作；`a`（小写，具体取值），表示某一确定动作。
   - **奖励（Reward）**：`R_{t+1}`（大写，随机变量），表示在时刻`t`的状态`S_t`采取动作`A_t`后，时刻`t+1`获得的奖励（注：部分场景会简写为`R_t`，仅为习惯差异，无数学本质区别）。
   - **折扣率（Discount Rate）**：`γ`（`0≤γ≤1`），用于计算折扣回报，体现未来奖励的衰减程度。

2. **单步交互过程**
   - 流程：`S_t`（当前状态）→`A_t`（采取动作）→`R_{t+1}`（获得奖励）→`S_{t+1}`（转移到下一状态）。
   - 随机性来源：
     - 动作选择：由**策略（Policy）** `π(a|s)`决定（状态`s`下采取动作`a`的概率）。
     - 奖励获取：由**奖励概率** `p(r|s,a)`决定（状态`s`采取动作`a`后获得奖励`r`的概率）。
     - 状态转移：由**状态转移概率** `p(s'|s,a)`决定（状态`s`采取动作`a`后转移到状态`s'`的概率）。

## 核心概念：State Value（状态价值）

### （一）定义与数学表达
1. **前置概念：折扣回报（Discounted Return）**
   - 定义：从时刻`t`的状态`S_t`出发，沿轨迹获得的所有奖励经折扣后的总和，记为`G_t`（随机变量，因轨迹随机性变化）。
   - 公式：`G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + … = Σ（从k=0到∞）γ^k R_{t+1+k}`。

2. **State Value的定义**
   - 本质：**折扣回报`G_t`的条件期望**，即从某一确定状态`s`出发，遵循策略`π`时，所有可能轨迹的折扣回报的平均值。
   - 全称：State-Value Function（状态价值函数），简称为状态价值。
   - 数学符号：`v_π(s)`（下标`π`表示依赖策略，括号内`s`表示当前状态）。
   - 公式：`v_π(s) = E[G_t | S_t = s]`（`E[·]`表示期望，`S_t = s`表示“时刻`t`状态为确定值`s`”的条件）。

### （二）State Value的核心属性
1. **双依赖特性**
   - 依赖状态`s`：不同状态的初始条件不同，轨迹与回报不同，状态价值也不同（如“靠近目标的状态”价值通常高于“远离目标的状态”）。
   - 依赖策略`π`：不同策略引导的动作选择不同，导致轨迹与回报差异，状态价值也不同（如“最优策略”下的状态价值高于“随机策略”）。
   - 补充表示：可写为`v(s, π)`（明确体现对`s`和`π`的依赖），简化后常用`v_π(s)`。

2. **价值含义**
   - 数值意义：`v_π(s)`越大，代表从状态`s`出发遵循策略`π`时，长期获得的平均回报越高，即该状态“越有价值”。
   - 作用：量化状态优劣，为策略评估（判断策略好坏）和策略优化（寻找更优策略）提供核心依据。


## 三、关键区分：Return（回报）与State Value（状态价值）
| 对比维度 | Return（折扣回报）                                           | State Value（状态价值）                  |
| -------- | ------------------------------------------------------------ | ---------------------------------------- |
| 计算对象 | 单个轨迹（一条具体的交互序列）                               | 多个轨迹（从同一状态出发的所有可能轨迹） |
| 随机性   | 随机变量（随轨迹不同而变化）                                 | 确定值（对随机回报的平均，消除随机性）   |
| 依赖因素 | 仅依赖具体轨迹                                               | 依赖状态`s`和策略`π`                     |
| 特殊情况 | 当环境与策略均为**确定性**时（仅一条轨迹），Return等于对应状态的State Value | -                                        |


## 四、实例验证：3种策略下的s1状态价值计算
基于前序课程的“网格世界”场景（含目标区域、禁止区域），3种策略仅`s1`状态的动作不同，其他状态策略一致，计算`s1`的`v_π(s1)`：

| 策略 | 策略描述（s1状态动作） | 轨迹特点                  | 状态价值计算过程                                             | 结果                    |
| ---- | ---------------------- | ------------------------- | ------------------------------------------------------------ | ----------------------- |
| π₁   | 确定性向下走           | 仅1条轨迹（避禁止区域）   | 轨迹回报=γ/(1-γ)（前序课程已计算），因仅1条轨迹，期望=回报本身 | v_π₁(s1)=γ/(1-γ)        |
| π₂   | 确定性向右走           | 仅1条轨迹（必进禁止区域） | 轨迹回报=-1 + γ/(1-γ)，因仅1条轨迹，期望=回报本身            | v_π₂(s1)=-1 + γ/(1-γ)   |
| π₃   | 50%概率向右、50%向下   | 2条可能轨迹（随机风险）   | 期望=0.5×（π₂轨迹回报） + 0.5×（π₁轨迹回报）=0.5×(-1 + γ/(1-γ)) + 0.5×(γ/(1-γ)) | v_π₃(s1)=-0.5 + γ/(1-γ) |

- 结论：`v_π₁(s1) > v_π₃(s1) > v_π₂(s1)`，与“策略1最优、策略2最差、策略3居中”的直观判断完全一致，验证了状态价值的量化评估作用。